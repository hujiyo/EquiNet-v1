'''
è®­ç»ƒè„šæœ¬

è¯„åˆ†åˆ¶åº¦ï¼ˆæ”¶ç›Šç‡åˆ¶åº¦ï¼Œä»¥ä»£ç å®ç°ä¸ºå‡†ï¼‰ï¼š
é‡‡ç”¨æ’åºèƒ½åŠ›è¯„ä¼°ï¼Œæ›´è´´è¿‘çœŸå®é€‰è‚¡åœºæ™¯ã€‚
æŒ‰é¢„æµ‹æ¦‚ç‡ä»é«˜åˆ°ä½æ’åºï¼Œç»Ÿè®¡Top-K%æ ·æœ¬çš„æ”¶ç›Šï¼š
æ¯ä¸ªåŒºé—´ç»Ÿè®¡ï¼šæ ·æœ¬æ•°ã€å¹³å‡æ”¶ç›Šã€ç´¯è®¡æ”¶ç›Šã€ä¸Šæ¶¨å‡†ç¡®ç‡ã€éè´Ÿç‡
'''

import os,torch,torch.nn as nn,torch.optim as optim,pandas as pd,numpy as np
import random
import math
import torch.nn.functional as F
from sklearn.metrics import roc_auc_score
from config import (ModelConfig, TrainingConfig, DataConfig,
                   DeviceConfig, ModelSaveConfig,
                   print_config_summary)

# å­¦ä¹ ç‡é¢„çƒ­è°ƒåº¦å™¨
class WarmupScheduler:
    """
    å­¦ä¹ ç‡é¢„çƒ­è°ƒåº¦å™¨
    åœ¨å‰å‡ è½®è®­ç»ƒä¸­ï¼Œå­¦ä¹ ç‡ä»å¾ˆå°çš„å€¼é€æ­¥å¢åŠ åˆ°ç›®æ ‡å­¦ä¹ ç‡
    è¿™æœ‰åŠ©äºæ¨¡å‹åœ¨è®­ç»ƒåˆæœŸæ›´ç¨³å®šåœ°æ”¶æ•›
    """
    def __init__(self, optimizer, warmup_epochs, target_lr, start_lr=None):
        """
        Args:
            optimizer: PyTorchä¼˜åŒ–å™¨
            warmup_epochs: é¢„çƒ­è½®æ•°
            target_lr: ç›®æ ‡å­¦ä¹ ç‡ï¼ˆé¢„çƒ­ç»“æŸåçš„å­¦ä¹ ç‡ï¼‰
            start_lr: é¢„çƒ­èµ·å§‹å­¦ä¹ ç‡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨target_lrçš„1/100
        """
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.target_lr = target_lr
        self.start_lr = start_lr if start_lr is not None else target_lr / 100
        self.current_epoch = 0
        
        # è®¾ç½®åˆå§‹å­¦ä¹ ç‡ä¸ºé¢„çƒ­èµ·å§‹å­¦ä¹ ç‡
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = self.start_lr
    
    def step(self, epoch=None):
        """
        æ›´æ–°å­¦ä¹ ç‡
        Args:
            epoch: å½“å‰è½®æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å†…éƒ¨è®¡æ•°å™¨
        """
        if epoch is not None:
            self.current_epoch = epoch
        else:
            self.current_epoch += 1
        
        if self.current_epoch < self.warmup_epochs:
            # é¢„çƒ­é˜¶æ®µï¼šçº¿æ€§å¢åŠ å­¦ä¹ ç‡
            lr = self.start_lr + (self.target_lr - self.start_lr) * ((self.current_epoch + 1) / self.warmup_epochs)
        else:
            # é¢„çƒ­ç»“æŸåä¿æŒç›®æ ‡å­¦ä¹ ç‡
            lr = self.target_lr
        
        # æ›´æ–°ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
        
        return lr
    
    def get_last_lr(self):
        """è·å–å½“å‰å­¦ä¹ ç‡ï¼ˆå…¼å®¹PyTorchè°ƒåº¦å™¨æ¥å£ï¼‰"""
        return [param_group['lr'] for param_group in self.optimizer.param_groups]
    
    def is_warmup_phase(self):
        """åˆ¤æ–­æ˜¯å¦è¿˜åœ¨é¢„çƒ­é˜¶æ®µ"""
        return self.current_epoch < self.warmup_epochs

# åŠ¨æ€åŠ æƒBCEæŸå¤±å‡½æ•°å®ç°
class DynamicWeightedBCE(nn.Module):
    """
    åŠ¨æ€åŠ æƒBCEæŸå¤±å‡½æ•°ï¼šæŒ‰æ ‡ç­¾æ¡¶åˆ†é…æƒé‡
    - æ ‡ç­¾1.0å›ºå®šæƒé‡4.0
    - æ ‡ç­¾0.6/0.3/0.0æŒ‰æ ·æœ¬æ•°é‡åŠ¨æ€åˆ†é…æƒé‡ï¼ˆæ ·æœ¬å°‘=æƒé‡é«˜ï¼‰
    """
    def __init__(self, pos_weight=4.0, reduction='mean'):
        super(DynamicWeightedBCE, self).__init__()
        self.reduction = reduction
        
        # å›ºå®šæ­£æ ·æœ¬æƒé‡
        self.register_buffer('pos_weight', torch.tensor(pos_weight))
        
        # åŠ¨æ€è´Ÿæ ·æœ¬æƒé‡ï¼ˆæŒ‰æ ‡ç­¾æ¡¶åˆ†é…ï¼‰
        self.register_buffer('weight_0_6', torch.tensor(1.0))
        self.register_buffer('weight_0_3', torch.tensor(1.0))
        self.register_buffer('weight_0_0', torch.tensor(1.0))
        
    def update_weights(self, targets):
        """
        äºŒåˆ†ç±»åŠ¨æ€æƒé‡ï¼šæ ¹æ®æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹åŠ¨æ€è°ƒæ•´
        targets: [batch_size] æ ‡ç­¾ (1.0/0.0)
        """
        if isinstance(targets, torch.Tensor):
            # BF16éœ€è¦å…ˆè½¬ä¸ºFP32å†è½¬numpy
            targets = targets.float().cpu().numpy()
        
        # ç»Ÿè®¡æ­£è´Ÿæ ·æœ¬æ•°é‡
        count_positive = np.sum(targets >= 0.5)  # ä¸Šæ¶¨æ ·æœ¬ï¼ˆâ‰¥5%ï¼‰
        count_negative = np.sum(targets < 0.5)   # ä¸ä¸Šæ¶¨æ ·æœ¬ï¼ˆ<5%ï¼‰
        
        if count_positive > 0 and count_negative > 0:
            # åŠ¨æ€è°ƒæ•´è´Ÿæ ·æœ¬æƒé‡ï¼Œä¿æŒæ­£è´Ÿæ ·æœ¬å¯¹æ€»æŸå¤±çš„è´¡çŒ®å¹³è¡¡
            # neg_weight = pos_weight * (æ­£æ ·æœ¬æ•° / è´Ÿæ ·æœ¬æ•°)
            neg_weight = float(self.pos_weight) * (count_positive / count_negative)
            
            # æ›´æ–°è´Ÿæ ·æœ¬æƒé‡ï¼ˆå¤ç”¨weight_0_0å˜é‡ï¼‰
            self.weight_0_0 = torch.tensor(neg_weight)
        elif count_positive == 0:
            # æ²¡æœ‰æ­£æ ·æœ¬ï¼Œè´Ÿæ ·æœ¬æƒé‡è®¾ä¸ºæ­£æ ·æœ¬æƒé‡
            self.weight_0_0 = torch.tensor(float(self.pos_weight))
        else:
            # æ²¡æœ‰è´Ÿæ ·æœ¬ï¼Œæƒé‡è®¾ä¸ºè¾ƒå°å€¼
            self.weight_0_0 = torch.tensor(0.1)
        
    def forward(self, inputs, targets):
        """
        inputs: [batch_size, 1] æ¨¡å‹è¾“å‡ºçš„logits (BF16)
        targets: [batch_size] çœŸå®æ ‡ç­¾ (1.0/0.0) (BF16)
        """
        # ç¡®ä¿è¾“å…¥å½¢çŠ¶æ­£ç¡®
        if inputs.dim() == 1:
            inputs = inputs.unsqueeze(1)
        
        inputs = inputs.squeeze()
        
        # è®¡ç®—BCE lossï¼ˆå¸¦logitsï¼‰
        # sigmoid(x) çš„æ•°å€¼ç¨³å®šè®¡ç®—
        max_val = torch.clamp(inputs, min=0)
        loss = inputs - inputs * targets + max_val + torch.log(torch.exp(-max_val) + torch.exp(-inputs - max_val))
        
        # äºŒåˆ†ç±»åŠ¨æ€æƒé‡ï¼šæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬åˆ†åˆ«ä½¿ç”¨åŠ¨æ€æƒé‡
        pos_weight = self.pos_weight.to(dtype=inputs.dtype, device=inputs.device)
        neg_weight = self.weight_0_0.to(dtype=inputs.dtype, device=inputs.device)
        
        # æ ¹æ®æ ‡ç­¾åˆ†é…æƒé‡ï¼šæ­£æ ·æœ¬ç”¨pos_weightï¼Œè´Ÿæ ·æœ¬ç”¨åŠ¨æ€neg_weight
        weights = torch.where(targets >= 0.5, pos_weight, neg_weight)
        loss = loss * weights
        
        # ğŸ”¥ æ–°å¢ï¼šå¯¹é¢„æµ‹åå·®è¾ƒå¤§çš„æ ·æœ¬è¿›è¡ŒæŒ‡æ•°çº§é¢å¤–æƒ©ç½š
        # è®¡ç®—é¢„æµ‹æ¦‚ç‡å€¼
        predictions = torch.sigmoid(inputs)  # å°†logitsè½¬ä¸ºæ¦‚ç‡ [0, 1]
        
        # è®¡ç®—é¢„æµ‹å€¼ä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„ç»å¯¹å·®å€¼
        prediction_error = torch.abs(predictions - targets)
        
        # å½“å·®å€¼ >= 0.15æ—¶ï¼Œåº”ç”¨æŒ‡æ•°çº§æƒ©ç½šï¼ˆé˜ˆå€¼ä»0.2é™ä½åˆ°0.15ï¼‰
        # ä½¿ç”¨ 3^(1.5Ã—å·®å€¼) ä½œä¸ºé¢å¤–æƒ©ç½šå› å­ï¼ˆåº•æ•°ä»2æå‡åˆ°3ï¼ŒæŒ‡æ•°æ”¾å¤§1.5å€ï¼‰
        # ä¾‹å¦‚ï¼šå·®å€¼0.2 -> 3^0.3 â‰ˆ 1.39 (æ¸©å’Œæƒ©ç½š)
        #       å·®å€¼0.5 -> 3^0.75 â‰ˆ 2.28 (ä¸­ç­‰æƒ©ç½š)
        #       å·®å€¼0.8 -> 3^1.2 â‰ˆ 3.74 (å¼ºæƒ©ç½š)
        #       å·®å€¼1.0 -> 3^1.5 â‰ˆ 5.20 (æŸå¤±æ”¾å¤§5å€ï¼)
        penalty_multiplier = torch.where(
            prediction_error >= 0.15,
            torch.pow(3.0, prediction_error * 1.5),   # æŒ‡æ•°çº§æƒ©ç½šï¼š3^(1.5Ã—å·®å€¼)
            torch.ones_like(prediction_error)         # å·®å€¼<0.15æ—¶ï¼Œæƒ©ç½šå› å­ä¸º1ï¼ˆä¸é¢å¤–æƒ©ç½šï¼‰
        )
        
        # åº”ç”¨é¢å¤–æƒ©ç½š
        loss = loss * penalty_multiplier
        
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss

class RMSNorm(nn.Module):
    """
    RMSNorm: åªåšç¼©æ”¾ï¼Œä¸å‡å‡å€¼
    ç›¸æ¯”LayerNormï¼Œä¿ç•™äº†ç‰¹å¾é—´çš„ç›¸å¯¹å¤§å°å…³ç³»
    è¿™å¯¹äºOHLCä»·æ ¼ç‰¹å¾å¾ˆé‡è¦ï¼Œå› ä¸º High > Close > Open > Low çš„å…³ç³»éœ€è¦ä¿æŒ
    """
    def __init__(self, dim, eps=1e-6):
        super(RMSNorm, self).__init__()
        self.scale = nn.Parameter(torch.ones(dim))
        self.eps = eps
    
    def forward(self, x):
        # è®¡ç®—RMS (Root Mean Square)
        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)
        # åªåšç¼©æ”¾ï¼Œä¸å‡å‡å€¼
        return x / rms * self.scale

class PositionalEncoding(nn.Module):
    """
    æ ‡å‡†çš„æ­£å¼¦ä½ç½®ç¼–ç 
    è®© Transformer è‡ªå·±å­¦ä¹ æ—¶é—´ä¾èµ–å…³ç³»ï¼Œä¸åŠ äººä¸ºè§„åˆ™
    """
    def __init__(self, d_model, max_seq_len=DataConfig.CONTEXT_LENGTH):
        super(PositionalEncoding, self).__init__()
        
        # åˆ›å»ºæ ‡å‡†çš„æ­£å¼¦/ä½™å¼¦ä½ç½®ç¼–ç 
        pe = torch.zeros(max_seq_len, d_model)
        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        self.register_buffer('pe', pe)
        
    def forward(self, x):
        # ç›´æ¥æ·»åŠ ä½ç½®ç¼–ç ï¼Œä¸ä½¿ç”¨LayerNormï¼ˆä¼šåœ¨åç»­å±‚ä¸­ä½¿ç”¨Pre-Normï¼‰
        seq_len = x.size(1)
        pe_slice = self.pe[:seq_len, :].unsqueeze(0)
        return x + pe_slice

class MultiHeadAttention(nn.Module):
    """
    æ ‡å‡†çš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆPre-Normæ¶æ„ï¼‰
    è®©æ¨¡å‹è‡ªåŠ¨å­¦ä¹ æ¯ä¸ªå¤´åº”è¯¥å…³æ³¨ä»€ä¹ˆç‰¹å¾ï¼Œä¸äººä¸ºå¹²é¢„
    """
    def __init__(self, d_model, nhead):
        super(MultiHeadAttention, self).__init__()
        self.d_model = d_model
        self.nhead = nhead
        
        assert d_model % nhead == 0
        
        # ä½¿ç”¨æ ‡å‡†çš„MultiheadAttention
        self.attention = nn.MultiheadAttention(d_model, nhead, batch_first=True)
        
        # Pre-Norm: åœ¨æ³¨æ„åŠ›ä¹‹å‰è¿›è¡Œå½’ä¸€åŒ–ï¼ˆä½¿ç”¨RMSNormä¿ç•™ç‰¹å¾ç›¸å¯¹å…³ç³»ï¼‰
        self.norm = RMSNorm(d_model)
        self.dropout = nn.Dropout(ModelConfig.ATTENTION_DROPOUT)
        
    def forward(self, x, attn_mask=None):
        # Pre-Normæ¶æ„ï¼šå…ˆå½’ä¸€åŒ–ï¼Œå†è®¡ç®—æ³¨æ„åŠ›ï¼Œæœ€åæ®‹å·®è¿æ¥
        # è¾“å‡º = è¾“å…¥ + Dropout(Attention(LayerNorm(è¾“å…¥)))
        
        mask = None
        if attn_mask is not None:
            mask = attn_mask.to(dtype=x.dtype, device=x.device)

        # Pre-Norm: å…ˆå¯¹è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–
        normalized_x = self.norm(x)
        
        # è®¡ç®—æ³¨æ„åŠ›
        attn_output, _ = self.attention(normalized_x, normalized_x, normalized_x, attn_mask=mask)
        
        # æ®‹å·®è¿æ¥ï¼ˆæ³¨æ„è¿™é‡Œæ˜¯åŠ åˆ°åŸå§‹è¾“å…¥xä¸Šï¼Œè€Œä¸æ˜¯normalized_xï¼‰
        output = x + self.dropout(attn_output)
        return output

class TransformerLayer(nn.Module):
    """
    æ ‡å‡†çš„ Transformer å±‚ï¼ˆPre-Normæ¶æ„ï¼‰
    è®¾è®¡ç†å¿µï¼šè®©æ¨¡å‹è‡ªåŠ¨å­¦ä¹ åº”è¯¥å…³æ³¨ä»€ä¹ˆç‰¹å¾ï¼Œä¸åŠ äººä¸ºå¹²é¢„
    Pre-Normç›¸æ¯”Post-Normæœ‰æ›´å¥½çš„è®­ç»ƒç¨³å®šæ€§
    """
    def __init__(self, d_model, nhead, use_ffn=True):
        super(TransformerLayer, self).__init__()
        
        self.use_ffn = use_ffn
        
        # ä½¿ç”¨Pre-Normå¤šå¤´æ³¨æ„åŠ›
        self.attention = MultiHeadAttention(d_model, nhead)
        
        if self.use_ffn:
            # å‰é¦ˆç½‘ç»œï¼Œç”¨äºè¿›ä¸€æ­¥å¤„ç†æ³¨æ„åŠ›çš„è¾“å‡º
            self.feed_forward = nn.Sequential(
                nn.Linear(d_model, d_model * 4),  # å…ˆæ‰©å±•ç»´åº¦
                nn.ReLU(),                        # æ¿€æ´»å‡½æ•°
                nn.Dropout(ModelConfig.DROPOUT_RATE),  # é˜²è¿‡æ‹Ÿåˆ
                nn.Linear(d_model * 4, d_model),  # å†å‹ç¼©å›åŸç»´åº¦
            )
            
            # Pre-Norm: åœ¨å‰é¦ˆç½‘ç»œä¹‹å‰è¿›è¡Œå½’ä¸€åŒ–ï¼ˆä½¿ç”¨RMSNormä¿ç•™ç‰¹å¾ç›¸å¯¹å…³ç³»ï¼‰
            self.norm = RMSNorm(d_model)
            self.dropout = nn.Dropout(ModelConfig.DROPOUT_RATE)
        
    def forward(self, x):
        # xçš„shape: [batch_size, seq_len, d_model]
        
        # Pre-Normæ¶æ„çš„æ³¨æ„åŠ›å­å±‚ï¼ˆMultiHeadAttentionå†…éƒ¨å·²ç»å®ç°äº†Pre-Normï¼‰
        # è¾“å‡º = è¾“å…¥ + Dropout(Attention(LayerNorm(è¾“å…¥)))
        x = self.attention(x, attn_mask=None)
        
        if self.use_ffn:
            # Pre-Normæ¶æ„çš„å‰é¦ˆç½‘ç»œå­å±‚
            # è¾“å‡º = è¾“å…¥ + Dropout(FFN(LayerNorm(è¾“å…¥)))
            normalized_x = self.norm(x)
            ff_out = self.feed_forward(normalized_x)
            x = x + self.dropout(ff_out)
        
        return x

class EnhancedStockTransformer(nn.Module):
    """
    æ”¹è¿›çš„ Transformer æ¨¡å‹ï¼ˆPre-Normæ¶æ„ + åˆ†ç¦»Embedding + æ¸è¿›å¼FFNï¼‰
    
    æ ¸å¿ƒæ”¹è¿›1ï¼šåˆ†ç¦»Embedding - é¿å…LayerNormæ—¶äº’ç›¸å¹²æ‰°
    - ä»·æ ¼ç‰¹å¾(OHLC 4ç»´) -> Embedding -> 48ç»´ (å 75%ï¼Œä¸»å¯¼åœ°ä½)
    - æˆäº¤é‡ç‰¹å¾(Volume 1ç»´) -> Embedding -> 16ç»´ (å 25%ï¼Œè¾…åŠ©ä¿¡æ¯)
    - æ‹¼æ¥åå¾—åˆ°64ç»´å‘é‡ï¼Œé€å…¥Transformer
    
    æ ¸å¿ƒæ”¹è¿›2ï¼šæ¸è¿›å¼FFN - åˆ†å±‚å­¦ä¹ ç­–ç•¥
    - Layer 1: åªç”¨Attentionï¼ˆä¸“æ³¨å­¦ä¹ æ—¶åºä¾èµ–å’Œç‰¹å¾å…³ç³»ï¼‰
    - Layer 2-5: Attention + FFNï¼ˆå¢åŠ éçº¿æ€§å˜æ¢èƒ½åŠ›ï¼‰
    - å¥½å¤„ï¼šç¬¬1å±‚çº¯ç²¹å­¦ä¹ æ¨¡å¼ï¼Œåç»­å±‚å¢å¼ºè¡¨è¾¾èƒ½åŠ›
    
    æ€»ä½“ä¼˜åŠ¿ï¼š
    1. é¿å…æˆäº¤é‡çš„å¤§å€¼ä¸»å¯¼LayerNormï¼Œæ‰­æ›²ä»·æ ¼ä¿¡å·
    2. ä¿æŒä»·æ ¼ç‰¹å¾ä¹‹é—´çš„ç›¸å¯¹å…³ç³»
    3. ä»·æ ¼ç‰¹å¾æœ‰æ›´å¤§çš„è¡¨è¾¾ç©ºé—´ï¼ˆ48ç»´ vs 16ç»´ï¼Œ3:1æ¯”ä¾‹ï¼‰
    4. æ¸è¿›å¼å­¦ä¹ ï¼šç¬¬1å±‚çº¯å­¦æ¨¡å¼ï¼Œåç»­å±‚å¢å¼ºè¡¨è¾¾
    5. å‚æ•°é‡å‡å°‘çº¦13%ï¼ˆç¬¬1å±‚çœæ‰FFNï¼‰ï¼Œç•¥å¾®é™ä½è¿‡æ‹Ÿåˆé£é™©
    """
    def __init__(self, input_dim, d_model, nhead, num_layers, output_dim, max_seq_len):
        super(EnhancedStockTransformer, self).__init__()
        
        # åˆ†ç¦»Embeddingï¼šä»·æ ¼å’Œæˆäº¤é‡ç‹¬ç«‹å¤„ç†
        self.price_embedding = nn.Linear(ModelConfig.PRICE_DIM, ModelConfig.PRICE_EMBED_DIM)
        self.volume_embedding = nn.Linear(ModelConfig.VOLUME_DIM, ModelConfig.VOLUME_EMBED_DIM)
        
        # ä½¿ç”¨æ ‡å‡†ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)
        
        # ç¬¬1å±‚åªç”¨Attentionï¼ˆä¸“æ³¨å­¦ä¹ åºåˆ—æ¨¡å¼ï¼‰
        # ç¬¬2-5å±‚ç”¨Attention+FFNï¼ˆå¢åŠ éçº¿æ€§å˜æ¢èƒ½åŠ›ï¼‰
        self.layers = nn.ModuleList([
            TransformerLayer(d_model, nhead, use_ffn=False) if i == 0 
            else TransformerLayer(d_model, nhead, use_ffn=True)
            for i in range(num_layers)
        ])
        
        # Pre-Normæ¶æ„ï¼šåœ¨æœ€åæ·»åŠ ä¸€ä¸ªRMSNorm
        # å› ä¸ºPre-Normçš„æœ€åä¸€å±‚æ²¡æœ‰å½’ä¸€åŒ–è¾“å‡º
        self.final_norm = RMSNorm(d_model)
        
        # ç®€åŒ–è¾“å‡ºå±‚ï¼Œå‡å°‘è¿‡æ‹Ÿåˆ
        self.output_projection = nn.Sequential(
            nn.Linear(d_model, d_model // 2),  # é™ç»´
            nn.ReLU(),
            nn.Dropout(ModelConfig.DROPOUT_RATE),
            nn.Linear(d_model // 2, output_dim)  # æœ€ç»ˆè¾“å‡º
        )
        
        self.dropout = nn.Dropout(ModelConfig.DROPOUT_RATE)
        
    def forward(self, x):
        # x: [batch_size, seq_len, 5] (OHLCV)
        
        # 1. åˆ†ç¦»Embeddingï¼šä»·æ ¼å’Œæˆäº¤é‡ç‹¬ç«‹å¤„ç†
        prices = x[:, :, :4]   # [batch_size, seq_len, 4] OHLC
        volumes = x[:, :, 4:5] # [batch_size, seq_len, 1] Volume
        
        price_emb = self.price_embedding(prices)      # [batch_size, seq_len, 48]
        volume_emb = self.volume_embedding(volumes)   # [batch_size, seq_len, 16]
        
        # 2. æ‹¼æ¥æˆ64ç»´ï¼ˆè€Œä¸æ˜¯ç›¸åŠ ï¼ï¼‰
        # è¿™æ ·ä»·æ ¼å’Œæˆäº¤é‡å„å æ®ç‹¬ç«‹çš„å­ç©ºé—´ï¼ŒLayerNormæ—¶å¹²æ‰°æœ€å°
        # ä»·æ ¼å 48ç»´(75%)ï¼Œæˆäº¤é‡å 16ç»´(25%)ï¼Œä»·æ ¼ä¸»å¯¼
        x = torch.cat([price_emb, volume_emb], dim=-1)  # [batch_size, seq_len, 64]
        
        # 3. ä½ç½®ç¼–ç 
        x = self.pos_encoding(x)
        x = self.dropout(x)
        
        # 4. Transformerå±‚ï¼ˆPre-Normæ¶æ„ï¼‰
        for layer in self.layers:
            x = layer(x)
        
        # 5. Pre-Normæ¶æ„éœ€è¦åœ¨æœ€åè¿›è¡Œå½’ä¸€åŒ–
        #    å› ä¸ºæ¯å±‚çš„è¾“å‡ºæ²¡æœ‰ç»è¿‡å½’ä¸€åŒ–
        x = self.final_norm(x)
        
        # 6. å–æœ€åæ—¶é—´æ­¥ + è¾“å‡ºæŠ•å½±
        last_hidden = x[:, -1, :]
        output = self.output_projection(last_hidden)
        
        return output

# å•ä¸ªæ–‡ä»¶å¤„ç†å‡½æ•°ï¼ˆç”¨äºå¤šè¿›ç¨‹ï¼‰
def process_single_file(args):
    """
    å¤„ç†å•ä¸ªæ–‡ä»¶ï¼Œè¿”å›åŸå§‹æ•°æ®ï¼ˆä¸åšå…¨å±€æ ‡å‡†åŒ–ï¼Œé¿å…æ•°æ®æ³„éœ²ï¼‰
    æŒ‰æ—¶é—´åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼šæœ€è¿‘80å¤©ä½œä¸ºæµ‹è¯•é›†ï¼Œå…¶ä½™ä½œä¸ºè®­ç»ƒé›†
    """
    file_path, file_name, test_days = args
    try:
        df = pd.read_excel(file_path, engine='openpyxl')
        # ä½¿ç”¨OHLCVï¼ˆ5ç»´ç‰¹å¾ï¼‰
        data = df[['start', 'max', 'min', 'end', 'volume']].values
        
        data_length = len(data)
        
        # æŒ‰æ—¶é—´åˆ’åˆ†ï¼šæœ€è¿‘test_dayså¤©ä½œä¸ºæµ‹è¯•é›†
        if data_length > test_days:
            train_split_point = data_length - test_days
            train_data = data[:train_split_point]  # å†å²æ•°æ®ä½œä¸ºè®­ç»ƒé›†
            test_data = data  # ä¿ç•™å…¨éƒ¨æ•°æ®ç”¨äºæµ‹è¯•é›†ï¼ˆéœ€è¦å‰é¢å†å²æ•°æ®ä½œä¸ºä¸Šä¸‹æ–‡ï¼‰
        else:
            # æ•°æ®ä¸è¶³ï¼Œåªèƒ½ç”¨ä½œè®­ç»ƒ
            train_data = data
            test_data = None
        
        stock_info = {
            'file_name': file_name,
            'data_length': data_length,
            'train_data': train_data,
            'test_data': test_data,
            'train_length': len(train_data) if train_data is not None else 0,
            'test_split_point': data_length - test_days if data_length > test_days else data_length
        }
        
        return stock_info
    except Exception as e:
        print(f"å¤„ç†æ–‡ä»¶ {file_name} æ—¶å‡ºé”™: {e}")
        return None

# æ•°æ®é¢„å¤„ç†å‡½æ•°ï¼ˆæŒ‰æ—¶é—´åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼‰
def load_and_preprocess_data(data_dir=DataConfig.DATA_DIR, test_days=DataConfig.TEST_DAYS):
    """
    æ•°æ®åŠ è½½å’Œé¢„å¤„ç†ï¼Œä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡ŒåŠ è½½
    æŒ‰æ—¶é—´åˆ’åˆ†ï¼šæ¯åªè‚¡ç¥¨çš„æœ€è¿‘test_dayså¤©ä½œä¸ºæµ‹è¯•é›†ï¼Œå…¶ä½™ä½œä¸ºè®­ç»ƒé›†
    """
    from multiprocessing import Pool, cpu_count
    
    all_files = [f for f in os.listdir(data_dir) if f.endswith('.xlsx')]
    all_files.sort()
    
    print(f"æ€»å…± {len(all_files)} åªè‚¡ç¥¨")
    print(f"åˆ’åˆ†ç­–ç•¥: æ¯åªè‚¡ç¥¨çš„æœ€è¿‘ {test_days} å¤©ä½œä¸ºæµ‹è¯•é›†ï¼Œå…¶ä½™ä½œä¸ºè®­ç»ƒé›†")
    
    # å¤„ç†æ‰€æœ‰æ–‡ä»¶
    file_args = [(os.path.join(data_dir, f), f, test_days) for f in all_files]
    num_workers = min(cpu_count(), 8)
    
    with Pool(num_workers) as pool:
        all_stock_info = [r for r in pool.map(process_single_file, file_args) if r is not None]
    
    # åˆ†ç¦»è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
    train_stock_info = []
    test_stock_info = []
    
    for stock_info in all_stock_info:
        # æ‰€æœ‰è‚¡ç¥¨çš„å†å²æ•°æ®éƒ½ç”¨äºè®­ç»ƒ
        if stock_info['train_data'] is not None and len(stock_info['train_data']) >= DataConfig.REQUIRED_LENGTH:
            train_stock_info.append({
                'file_name': stock_info['file_name'],
                'data': stock_info['train_data'],
                'data_length': stock_info['train_length']
            })
        
        # æœ‰è¶³å¤Ÿæ•°æ®çš„è‚¡ç¥¨ç”¨äºæµ‹è¯•
        if stock_info['test_data'] is not None and len(stock_info['test_data']) >= DataConfig.REQUIRED_LENGTH:
            test_stock_info.append({
                'file_name': stock_info['file_name'],
                'data': stock_info['test_data'],
                'data_length': len(stock_info['test_data']),
                'test_split_point': stock_info['test_split_point']  # æµ‹è¯•é›†èµ·å§‹ä½ç½®
            })
    
    print(f"è®­ç»ƒé›†: {len(train_stock_info)} åªè‚¡ç¥¨çš„å†å²æ•°æ®")
    print(f"æµ‹è¯•é›†: {len(test_stock_info)} åªè‚¡ç¥¨çš„æœ€è¿‘ {test_days} å¤©æ•°æ®")
    
    return train_stock_info, test_stock_info

# è®¡ç®—è‚¡ç¥¨é€‰æ‹©æƒé‡
def calculate_stock_weights(stock_info_list):
    """
    è®¡ç®—æ¯åªè‚¡ç¥¨çš„é‡‡æ ·æƒé‡
    æ•°æ®é‡è¶Šå¤§çš„è‚¡ç¥¨æƒé‡è¶Šå¤§ï¼Œä½†æœ€å¤§ä¸è¶…è¿‡å¹³å‡å€¼çš„1.5å€
    """
    data_lengths = [info['data_length'] for info in stock_info_list]
    avg_length = np.mean(data_lengths)
    
    # è®¡ç®—æƒé‡ï¼šæ•°æ®é•¿åº¦ / å¹³å‡é•¿åº¦ï¼Œä½†é™åˆ¶åœ¨1.0åˆ°1.5ä¹‹é—´
    weights = []
    for length in data_lengths:
        weight = length / avg_length
        weight = max(1.0, min(1.5, weight))  # é™åˆ¶åœ¨1.0åˆ°1.5ä¹‹é—´
        weights.append(weight)
    
    # å½’ä¸€åŒ–æƒé‡ï¼Œä½¿å…¶æ€»å’Œä¸º1.0ï¼ˆnp.random.choiceè¦æ±‚ï¼‰
    total_weight = sum(weights)
    normalized_weights = [w / total_weight for w in weights]
    
    return normalized_weights

# æ”¹è¿›çš„æ ·æœ¬ç”Ÿæˆå‡½æ•°ï¼ˆä½¿ç”¨æ»šåŠ¨çª—å£æ ‡å‡†åŒ–ï¼Œé¿å…æ•°æ®æ³„éœ²ï¼‰
def generate_single_sample_improved(stock_info_list, stock_weights):
    """
    æ”¹è¿›çš„æ ·æœ¬ç”Ÿæˆå‡½æ•°ï¼ˆä¿®å¤æ•°æ®æ³„éœ²é—®é¢˜ï¼‰
    1. æ ¹æ®æ•°æ®é‡å¤§å°é€‰æ‹©è‚¡ç¥¨ï¼ˆæ•°æ®é‡å¤§çš„æ¦‚ç‡æ›´é«˜ï¼‰
    2. éšæœºé€‰æ‹©è®­ç»ƒé›†æ—¶é—´èŒƒå›´å†…çš„æ—¶é—´çª—å£
    3. ä½¿ç”¨æ»šåŠ¨çª—å£æ ‡å‡†åŒ–ï¼šåªä½¿ç”¨å½“å‰æ ·æœ¬çš„å†å²æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–
    """
    for _ in range(100):  # æœ€å¤šå°è¯•100æ¬¡ç”Ÿæˆæœ‰æ•ˆæ ·æœ¬
        # ç¬¬ä¸€æ­¥ï¼šæ ¹æ®æƒé‡é€‰æ‹©è‚¡ç¥¨
        stock_index = np.random.choice(len(stock_info_list), p=stock_weights)
        stock_info = stock_info_list[stock_index]
        stock_data = stock_info['data']  # è®­ç»ƒé›†æ•°æ®ï¼ˆå·²ç»æŒ‰æ—¶é—´åˆ’åˆ†ï¼‰
        
        context_length = DataConfig.CONTEXT_LENGTH
        required_length = DataConfig.REQUIRED_LENGTH
        
        if len(stock_data) < required_length:
            continue
            
        # ç¬¬äºŒæ­¥ï¼šåœ¨è®­ç»ƒé›†èŒƒå›´å†…éšæœºé€‰æ‹©èµ·å§‹ä½ç½®
        # æ³¨æ„ï¼šstart_index å¿…é¡» > 0ï¼Œå› ä¸ºéœ€è¦å‰ä¸€å¤©çš„æ•°æ®æ¥è®¡ç®—æ¶¨è·Œå¹…
        max_start_index = len(stock_data) - required_length
        if max_start_index < 1:
            continue  # æ•°æ®ä¸è¶³ï¼Œè‡³å°‘éœ€è¦ required_length + 1 å¤©
        start_index = np.random.randint(1, max_start_index + 1)
        
        # æå–åŸå§‹æ•°æ®çª—å£
        input_seq_raw = stock_data[start_index:start_index + context_length]
        
        # ğŸ”‘ ç‰¹å¾æ ‡å‡†åŒ–ï¼šè®¡ç®—æ¯å¤©ç›¸å¯¹å‰ä¸€å¤©çš„æ¶¨è·Œå¹…
        # è¿™æ˜¯æœ€ç¬¦åˆäººç±»äº¤æ˜“æ€ç»´çš„æ–¹å¼ï¼šä»Šå¤©ç›¸æ¯”æ˜¨å¤©æ¶¨äº†å¤šå°‘
        
        # åˆå§‹åŒ–æ ‡å‡†åŒ–åçš„æ•°æ®
        input_seq = np.zeros_like(input_seq_raw, dtype=np.float64)
        
        # è·å–çª—å£å‰ä¸€å¤©çš„æ•°æ®ä½œä¸ºç¬¬1å¤©çš„åŸºå‡†
        prev_day_data = stock_data[start_index - 1]
        prev_prices = prev_day_data[:4]  # OHLC
        prev_volume = prev_day_data[4]   # Volume
        
        # é¿å…é™¤é›¶é”™è¯¯
        if np.any(prev_prices == 0) or prev_volume == 0:
            continue
        
        # ç¬¬1å¤©ï¼šç›¸å¯¹äºçª—å£å‰ä¸€å¤©çš„æ”¶ç›˜ä»·ï¼ˆä»·æ ¼ç‰¹å¾ï¼‰
        prev_close = prev_prices[3]  # å‰ä¸€å¤©çš„æ”¶ç›˜ä»·
        if prev_close == 0:
            continue
        input_seq[0, :4] = (input_seq_raw[0, :4] - prev_close) / prev_close
        # æˆäº¤é‡ç‰¹å¾ï¼šç›´æ¥ä½¿ç”¨ç›¸å¯¹å˜åŒ–æ¯”ä¾‹
        input_seq[0, 4] = (input_seq_raw[0, 4] - prev_volume) / prev_volume
        
        # ç¬¬2-40å¤©ï¼šç›¸å¯¹äºå‰ä¸€å¤©çš„æ”¶ç›˜ä»·
        for i in range(1, context_length):
            # ä»·æ ¼ç‰¹å¾ï¼šæ‰€æœ‰ä»·æ ¼(OHLC)éƒ½ç›¸å¯¹äºå‰ä¸€å¤©çš„æ”¶ç›˜ä»·
            # è¿™ç¬¦åˆçœŸå®äº¤æ˜“é€»è¾‘ï¼šä»Šå¤©çš„å¼€ç›˜/æœ€é«˜/æœ€ä½/æ”¶ç›˜éƒ½å’Œæ˜¨å¤©æ”¶ç›˜ä»·æ¯”
            yesterday_close = input_seq_raw[i-1, 3]  # å‰ä¸€å¤©çš„æ”¶ç›˜ä»·
            yesterday_volume = input_seq_raw[i-1, 4]  # å‰ä¸€å¤©çš„æˆäº¤é‡
            if yesterday_close == 0 or yesterday_volume == 0:
                # å¦‚æœæ˜¨å¤©æ”¶ç›˜ä»·æˆ–æˆäº¤é‡ä¸º0ï¼Œè·³è¿‡è¿™ä¸ªæ ·æœ¬
                break
            input_seq[i, :4] = (input_seq_raw[i, :4] - yesterday_close) / yesterday_close
            # æˆäº¤é‡ç‰¹å¾ï¼šç›´æ¥ä½¿ç”¨ç›¸å¯¹å˜åŒ–æ¯”ä¾‹
            input_seq[i, 4] = (input_seq_raw[i, 4] - yesterday_volume) / yesterday_volume
        else:
            # åªæœ‰forå¾ªç¯æ­£å¸¸ç»“æŸï¼ˆæ²¡æœ‰breakï¼‰æ‰ä¼šæ‰§è¡Œè¿™é‡Œ
            # è¿™è¡¨ç¤ºæ‰€æœ‰å†å²æ•°æ®éƒ½æˆåŠŸæ ‡å‡†åŒ–äº†
            
            # ç»Ÿä¸€ä½¿ç”¨æ—§çš„æ¶¨å¹…å‹æ ‡ç­¾ï¼šåŸºäºæœªæ¥æ¶¨å¹…å¤§å°
            original_start_price = stock_data[start_index + context_length - 1, 3]  # å½“å‰æ”¶ç›˜ä»·
            original_end_price = stock_data[start_index + DataConfig.REQUIRED_LENGTH - 1, 3]   # Nå¤©åæ”¶ç›˜ä»·

            if original_start_price == 0:  # é¿å…é™¤é›¶é”™è¯¯
                continue

            cumulative_return = (original_end_price - original_start_price) / original_start_price

            # è½¯æ ‡ç­¾æœºåˆ¶ï¼šé™ä½è¾¹ç•ŒåŒºåŸŸçš„æƒ©ç½š
            # - æ”¶ç›Š â‰¥ 8% â†’ 1.0ï¼ˆæ˜ç¡®ä¸Šæ¶¨ï¼‰
            # - æ”¶ç›Š 0-8% â†’ 0.4ï¼ˆè¾¹ç•ŒåŒºåŸŸï¼Œé™ä½çŸ›ç›¾æƒ©ç½šï¼‰
            # - æ”¶ç›Š < 0% â†’ 0.0ï¼ˆæ˜ç¡®ä¸æ¶¨ï¼‰
            if cumulative_return >= DataConfig.UPRISE_THRESHOLD:  # æ¶¨å¹…â‰¥é˜ˆå€¼
                target = 1.0
            elif cumulative_return >= 0:  # 0-8%ä¹‹é—´
                target = 0.4
            else:  # æ¶¨å¹…<0%
                target = 0.0

            return input_seq, target
    
    raise ValueError("æ— æ³•ç”Ÿæˆæœ‰æ•ˆæ ·æœ¬ï¼šè‚¡ç¥¨æ•°æ®é•¿åº¦ä¸è¶³æˆ–æ”¶ç›˜ä»·ä¸º0")

def generate_batch_samples_improved(stock_info_list, stock_weights, batch_size):
    """
    æ”¹è¿›çš„æ‰¹é‡ç”Ÿæˆè®­ç»ƒæ ·æœ¬ï¼ˆä½¿ç”¨æ»šåŠ¨çª—å£æ ‡å‡†åŒ–ï¼‰
    è¿”å›: (batch_inputs, batch_targets)
    batch_inputs: numpy array, shape [batch_size, context_length, 5]  
    batch_targets: numpy array, shape [batch_size]
    """
    batch_inputs = []
    batch_targets = []
    
    attempts = 0
    max_attempts = batch_size * 10  # é˜²æ­¢æ— é™å¾ªç¯
    
    while len(batch_inputs) < batch_size and attempts < max_attempts:
        attempts += 1
        try:
            input_seq, target = generate_single_sample_improved(stock_info_list, stock_weights)
            batch_inputs.append(input_seq)
            batch_targets.append(target)
        except ValueError:
            continue
    
    if len(batch_inputs) < batch_size:
        raise ValueError(f"æ— æ³•ç”Ÿæˆè¶³å¤Ÿçš„æ ·æœ¬ï¼Œåªç”Ÿæˆäº† {len(batch_inputs)}/{batch_size} ä¸ª")
    
    return np.array(batch_inputs), np.array(batch_targets)

# åˆ›å»ºå›ºå®šçš„è¯„ä¼°æ•°æ®é›†ï¼ˆä½¿ç”¨æ»šåŠ¨çª—å£æ ‡å‡†åŒ–ï¼Œåªä½¿ç”¨æµ‹è¯•é›†æ—¶é—´èŒƒå›´ï¼‰
def create_fixed_evaluation_dataset(test_stock_info, seed=DataConfig.RANDOM_SEED):
    """
    åˆ›å»ºå›ºå®šçš„è¯„ä¼°æ•°æ®é›†ï¼Œä½¿ç”¨æ»šåŠ¨çª—å£æ ‡å‡†åŒ–é¿å…æ•°æ®æ³„éœ²
    åªä½¿ç”¨æµ‹è¯•é›†çš„æ—¶é—´èŒƒå›´ï¼ˆæœ€è¿‘80å¤©ï¼‰ï¼Œä¸¥æ ¼æ—¶é—´åˆ†ç¦»
    ä½¿ç”¨å…¨éƒ¨æµ‹è¯•æ ·æœ¬è¿›è¡Œè¯„ä¼°ï¼Œç¡®ä¿è¯„ä¼°ç»“æœæ›´åŠ å‡†ç¡®å’Œç¨³å®š
    """
    print("æ­£åœ¨åˆ›å»ºå›ºå®šçš„è¯„ä¼°æ•°æ®é›†ï¼ˆä½¿ç”¨æ»šåŠ¨çª—å£æ ‡å‡†åŒ–ï¼Œä¸¥æ ¼æ—¶é—´åˆ†ç¦»ï¼‰...")
    # è®¾ç½®æ‰€æœ‰å¯èƒ½çš„éšæœºç§å­ä»¥ç¡®ä¿å®Œå…¨å¯é‡å¤
    np.random.seed(seed)
    random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    
    eval_inputs = []
    eval_targets = []
    eval_cumulative_returns = []  # å­˜å‚¨å®é™…æ¶¨è·Œå¹…
    
    # é¢„å…ˆç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„æ ·æœ¬
    all_possible_samples = []
    context_length = DataConfig.CONTEXT_LENGTH
    required_length = DataConfig.REQUIRED_LENGTH
    test_days = DataConfig.TEST_DAYS
    
    for stock_idx, stock_info in enumerate(test_stock_info):
        stock_data = stock_info['data']  # åŸå§‹æ•°æ®ï¼ˆåŒ…å«å…¨éƒ¨å†å²ï¼‰
        test_split_point = stock_info['test_split_point']  # æµ‹è¯•é›†èµ·å§‹ä½ç½®
        
        if len(stock_data) < required_length:
            continue
        
        # ğŸ”‘ å…³é”®ï¼šä¸¥æ ¼çš„æµ‹è¯•é›†åˆ’åˆ†ï¼Œé¿å…æ•°æ®æ³„éœ²
        # æµ‹è¯•é›†80å¤©ï¼š[test_split_point, len(stock_data))
        # æ¯åªè‚¡ç¥¨å¯ç”Ÿæˆçš„æµ‹è¯•æ ·æœ¬æ•° = æµ‹è¯•é›†å¤©æ•° - åºåˆ—é•¿åº¦ - é¢„æµ‹å¤©æ•° = 80 - 40 - 3 = 37ä¸ª
        
        # æœ€æ—©é¢„æµ‹æ—¶é—´ç‚¹ï¼šæµ‹è¯•é›†ç¬¬41å¤©ï¼ˆå‰40å¤©ä½œä¸ºä¸Šä¸‹æ–‡ï¼‰
        # æœ€æ™šé¢„æµ‹æ—¶é—´ç‚¹ï¼šæµ‹è¯•é›†å€’æ•°ç¬¬4å¤©ï¼ˆéœ€è¦é¢„ç•™3å¤©æœªæ¥æ•°æ®ï¼‰
        min_predict_point = test_split_point + context_length
        max_predict_point = len(stock_data) - DataConfig.FUTURE_DAYS - 1
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
        if min_predict_point > max_predict_point:
            continue  # æµ‹è¯•é›†ä¸å¤Ÿ80å¤©ï¼Œæ— æ³•ç”Ÿæˆæ ·æœ¬
        
        # å°†é¢„æµ‹æ—¶é—´ç‚¹è½¬æ¢ä¸ºstart_idx
        # é¢„æµ‹æ—¶é—´ç‚¹ = start_idx + context_length - 1
        # start_idx = é¢„æµ‹æ—¶é—´ç‚¹ - context_length + 1
        min_start_idx = min_predict_point - context_length + 1
        max_start_idx = max_predict_point - context_length + 1
        
        # ä¸ºæ¯åªè‚¡ç¥¨ç”Ÿæˆæµ‹è¯•æ ·æœ¬
        # æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹æ—¶é—´ç‚¹åœ¨æµ‹è¯•é›†æ—¶é—´èŒƒå›´å†…
        # ä¸Šä¸‹æ–‡ä½¿ç”¨çš„æ˜¯æµ‹è¯•é›†å‰éƒ¨åˆ†å¤©æ•°+è®­ç»ƒé›†æ•°æ®ï¼ˆå¦‚æœ‰éœ€è¦ï¼‰
        for start_idx in range(min_start_idx, max_start_idx + 1):
            
            # æå–åŸå§‹æ•°æ®çª—å£ï¼ˆå¯èƒ½åŒ…å«éƒ¨åˆ†è®­ç»ƒé›†æ•°æ®ä½œä¸ºä¸Šä¸‹æ–‡ï¼‰
            input_seq_raw = stock_data[start_idx:start_idx + context_length]
            
            # ğŸ”‘ ç‰¹å¾æ ‡å‡†åŒ–ï¼šè®¡ç®—æ¯å¤©ç›¸å¯¹å‰ä¸€å¤©çš„æ¶¨è·Œå¹…
            # åˆå§‹åŒ–æ ‡å‡†åŒ–åçš„æ•°æ®
            input_seq = np.zeros_like(input_seq_raw, dtype=np.float64)
            
            # ç¬¬1å¤©ï¼šä½¿ç”¨çª—å£å‰ä¸€å¤©çš„æ•°æ®ä½œä¸ºåŸºå‡†
            # æ³¨æ„ï¼šç†è®ºä¸Šstart_idxä¸å¯èƒ½ä¸º0ï¼ˆå› ä¸ºæµ‹è¯•é›†å‰é¢æœ‰è®­ç»ƒé›†æ•°æ®ï¼‰
            # ä½†ä¸ºäº†ä»£ç å¥å£®æ€§ï¼Œä»ç„¶æ£€æŸ¥
            if start_idx == 0:
                continue  # è·³è¿‡ï¼Œå› ä¸ºæ²¡æœ‰å‰ä¸€å¤©æ•°æ®
            
            prev_day_data = stock_data[start_idx - 1]
            prev_prices = prev_day_data[:4]  # OHLC
            prev_volume = prev_day_data[4]   # Volume
            
            # é¿å…é™¤é›¶é”™è¯¯
            if np.any(prev_prices == 0) or prev_volume == 0:
                continue
            
            # ç¬¬1å¤©ï¼šç›¸å¯¹äºçª—å£å‰ä¸€å¤©çš„æ”¶ç›˜ä»·ï¼ˆä»·æ ¼ç‰¹å¾ï¼‰
            prev_close = prev_prices[3]  # å‰ä¸€å¤©çš„æ”¶ç›˜ä»·
            if prev_close == 0:
                continue
            input_seq[0, :4] = (input_seq_raw[0, :4] - prev_close) / prev_close
            # æˆäº¤é‡ç‰¹å¾ï¼šç›´æ¥ä½¿ç”¨ç›¸å¯¹å˜åŒ–æ¯”ä¾‹
            input_seq[0, 4] = (input_seq_raw[0, 4] - prev_volume) / prev_volume
            
            # ç¬¬2-40å¤©ï¼šç›¸å¯¹äºå‰ä¸€å¤©çš„æ”¶ç›˜ä»·
            valid_sample = True
            for i in range(1, context_length):
                yesterday_close = input_seq_raw[i-1, 3]  # å‰ä¸€å¤©çš„æ”¶ç›˜ä»·
                yesterday_volume = input_seq_raw[i-1, 4]  # å‰ä¸€å¤©çš„æˆäº¤é‡
                
                if yesterday_close == 0 or yesterday_volume == 0:
                    valid_sample = False
                    break
                
                input_seq[i, :4] = (input_seq_raw[i, :4] - yesterday_close) / yesterday_close
                # æˆäº¤é‡ç‰¹å¾ï¼šç›´æ¥ä½¿ç”¨ç›¸å¯¹å˜åŒ–æ¯”ä¾‹
                input_seq[i, 4] = (input_seq_raw[i, 4] - yesterday_volume) / yesterday_volume
            
            if not valid_sample:
                continue
            
            # ç»Ÿä¸€ä½¿ç”¨æ¶¨å¹…å‹æ ‡ç­¾ï¼šåŸºäºæœªæ¥æ¶¨å¹…å¤§å°
            original_start_price = stock_data[start_idx + context_length - 1, 3]
            original_end_price = stock_data[start_idx + required_length - 1, 3]

            if original_start_price == 0:
                continue

            cumulative_return = (original_end_price - original_start_price) / original_start_price

            # æµ‹è¯•é›†äºŒåˆ†ç±»ï¼šä¸è®­ç»ƒé›†ä¿æŒä¸€è‡´
            if cumulative_return >= DataConfig.UPRISE_THRESHOLD:  # æ¶¨å¹…â‰¥é˜ˆå€¼
                target = 1.0
            else:  # æ¶¨å¹…<é˜ˆå€¼
                target = 0.0

            all_possible_samples.append((input_seq, target, stock_idx, start_idx, cumulative_return))
    
    print(f"æ€»å…±å¯ç”¨æ ·æœ¬: {len(all_possible_samples)} ä¸ª")
    
    # ä½¿ç”¨å…¨éƒ¨æ ·æœ¬è¿›è¡Œè¯„ä¼°ï¼ˆæ›´ç§‘å­¦ï¼Œä¸”è¯„ä¼°é€Ÿåº¦å¾ˆå¿«ï¼‰
    selected_samples = all_possible_samples
    print(f"ä½¿ç”¨å…¨éƒ¨ {len(selected_samples)} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°")
    
    # æŒ‰è‚¡ç¥¨ç´¢å¼•å’Œæ—¶é—´ç´¢å¼•æ’åºï¼Œç¡®ä¿é¡ºåºä¸€è‡´
    selected_samples.sort(key=lambda x: (x[2], x[3]))
    
    # åˆ†ç¦»è¾“å…¥å’Œæ ‡ç­¾
    for input_seq, target, stock_idx, start_idx, cumulative_return in selected_samples:
        eval_inputs.append(input_seq)
        eval_targets.append(target)
        eval_cumulative_returns.append(cumulative_return)
    
    eval_inputs = np.array(eval_inputs)
    eval_targets = np.array(eval_targets)
    eval_cumulative_returns = np.array(eval_cumulative_returns)
    
    # ä¿å­˜è¯„ä¼°æ ·æœ¬ä¿¡æ¯ä»¥ä¾¿è°ƒè¯•
    print(f"è¯„ä¼°æ ·æœ¬è¯¦ç»†ä¿¡æ¯:")
    print(f"  æ ·æœ¬æ€»æ•°: {len(eval_inputs)}")
    print(f"  æ¥è‡ªè‚¡ç¥¨æ•°: {len(set(s[2] for s in selected_samples))}")
    print(f"  æ—¶é—´çª—å£èŒƒå›´: {min(s[3] for s in selected_samples)} - {max(s[3] for s in selected_samples)}")
    
    # æ‰“å°ç±»åˆ«åˆ†å¸ƒ
    unique, counts = np.unique(eval_targets, return_counts=True)
    class_names = ['ä¸ä¸Šæ¶¨', 'ä¸Šæ¶¨']
    print("è¯„ä¼°é›†ç±»åˆ«åˆ†å¸ƒ:")
    for cls, count in zip(unique, counts):
        print(f"  {class_names[int(cls)]}: {count} ä¸ªæ ·æœ¬ ({count/len(eval_targets)*100:.1f}%)")
    
    # æ‰“å°æ”¶ç›Šç‡ç»Ÿè®¡
    print(f"\nçœŸå®æ”¶ç›Šç‡ç»Ÿè®¡:")
    print(f"  æœ€å°å€¼: {np.min(eval_cumulative_returns)*100:.2f}%")
    print(f"  æœ€å¤§å€¼: {np.max(eval_cumulative_returns)*100:.2f}%")
    print(f"  å¹³å‡å€¼: {np.mean(eval_cumulative_returns)*100:.2f}%")
    print(f"  ä¸­ä½æ•°: {np.median(eval_cumulative_returns)*100:.2f}%")
    print(f"  â‰¥0%æ ·æœ¬: {np.sum(eval_cumulative_returns >= 0)} ({np.sum(eval_cumulative_returns >= 0)/len(eval_cumulative_returns)*100:.1f}%)")
    print(f"  â‰¥3%æ ·æœ¬: {np.sum(eval_cumulative_returns >= 0.03)} ({np.sum(eval_cumulative_returns >= 0.03)/len(eval_cumulative_returns)*100:.1f}%)")
    print(f"  â‰¥10%æ ·æœ¬: {np.sum(eval_cumulative_returns >= 0.10)} ({np.sum(eval_cumulative_returns >= 0.10)/len(eval_cumulative_returns)*100:.1f}%)")
    
    return eval_inputs, eval_targets, eval_cumulative_returns

# æ‰¹é‡è¯„ä¼°å‡½æ•°
def evaluate_model_batch(model, eval_inputs, eval_targets, eval_cumulative_returns, device, batch_size=DataConfig.EVAL_BATCH_SIZE):
    """
    ä½¿ç”¨æ‰¹å¤„ç†è¿›è¡Œå¿«é€Ÿè¯„ä¼°ï¼ˆäºŒåˆ†ç±»ï¼‰
    è¿”å›: (total, class_correct, class_total, pred_positive_correct, pred_positive_total, pred_non_negative, auc_score, confidence_stats, top_percent_stats)
    
    top_percent_stats: æŒ‰é¢„æµ‹æ¦‚ç‡æ’åºåï¼Œå‰1%/5%/10%æ ·æœ¬çš„æ”¶ç›Šç»Ÿè®¡
    """
    model.eval()
    total = 0
    class_correct = [0, 0]  # [ä¸ä¸Šæ¶¨æ­£ç¡®æ•°, ä¸Šæ¶¨æ­£ç¡®æ•°]
    class_total = [0, 0]    # [ä¸ä¸Šæ¶¨æ€»æ•°, ä¸Šæ¶¨æ€»æ•°]
    
    # æ–°å¢ï¼šé¢„æµ‹ç»Ÿè®¡
    pred_positive_correct = 0  # é¢„æµ‹ä¸Šæ¶¨ä¸”æ­£ç¡®çš„æ•°é‡
    pred_positive_total = 0    # é¢„æµ‹ä¸Šæ¶¨çš„æ€»æ•°é‡
    pred_non_negative = 0       # é¢„æµ‹ä¸Šæ¶¨ä¸”å®é™…æ¶¨å¹…â‰¥0%çš„æ•°é‡
    
    # æ–°å¢ï¼šç”¨äºAUCè®¡ç®—å’ŒTop-Kæ’åºçš„åˆ—è¡¨
    all_probabilities = []
    all_targets = []
    all_returns = []  # å­˜å‚¨æ‰€æœ‰æ ·æœ¬çš„å®é™…æ”¶ç›Šç‡
    
    # æ–°å¢ï¼šç½®ä¿¡åº¦åŒºé—´ç»Ÿè®¡ {åŒºé—´åç§°: [é¢„æµ‹ä¸Šæ¶¨ä¸”æ­£ç¡®æ•°, é¢„æµ‹ä¸Šæ¶¨æ€»æ•°, é¢„æµ‹ä¸Šæ¶¨ä¸”å®é™…æ¶¨å¹…â‰¥0%æ•°]}
    confidence_stats = {
        '0.50-0.55': [0, 0, 0],
        '0.55-0.58': [0, 0, 0],
        '0.58-0.60': [0, 0, 0],
        '0.60-0.70': [0, 0, 0],
        '0.70-1.00': [0, 0, 0]
    }
    
    num_samples = len(eval_inputs)
    num_batches = (num_samples + batch_size - 1) // batch_size
    
    with torch.no_grad():
        for i in range(num_batches):
            start_idx = i * batch_size
            end_idx = min((i + 1) * batch_size, num_samples)
            
            # æ‰¹é‡å¤„ç† (ä½¿ç”¨BF16ç²¾åº¦)
            batch_inputs = torch.tensor(eval_inputs[start_idx:end_idx], 
                                      dtype=torch.bfloat16).to(device)
            batch_targets = eval_targets[start_idx:end_idx]
            batch_returns = eval_cumulative_returns[start_idx:end_idx]  # è·å–å®é™…æ¶¨è·Œå¹…
            
            # æ‰¹é‡æ¨ç†
            batch_outputs = model(batch_inputs)  # [batch_size, 1]
            # BF16éœ€è¦å…ˆè½¬ä¸ºFP32å†è½¬numpy
            batch_probabilities = torch.sigmoid(batch_outputs).float().cpu().numpy().flatten()
            batch_predictions = (batch_probabilities > 0.5).astype(int)  # æ¦‚ç‡>0.5é¢„æµ‹ä¸ºä¸Šæ¶¨
            
            # æ”¶é›†æ‰€æœ‰æ¦‚ç‡ã€æ ‡ç­¾å’Œæ”¶ç›Šç‡ç”¨äºåç»­è®¡ç®—
            all_probabilities.extend(batch_probabilities)
            all_targets.extend(batch_targets)
            all_returns.extend(batch_returns)
            
            # æ‰¹é‡è®¡ç®—å¾—åˆ†
            for j in range(len(batch_targets)):
                target = int(batch_targets[j])
                prediction = batch_predictions[j]
                actual_return = batch_returns[j]  # è·å–å®é™…æ¶¨è·Œå¹…
                probability = batch_probabilities[j]  # è·å–é¢„æµ‹æ¦‚ç‡
                
                class_total[target] += 1
                total += 1
                
                # ç»Ÿè®¡é¢„æµ‹ä¸Šæ¶¨çš„æƒ…å†µ
                if prediction == 1:
                    pred_positive_total += 1
                    if target == 1:  # é¢„æµ‹ä¸Šæ¶¨ä¸”å®é™…ä¸Šæ¶¨
                        pred_positive_correct += 1
                    if actual_return >= 0:  # é¢„æµ‹ä¸Šæ¶¨ä¸”å®é™…æ¶¨å¹…â‰¥0%
                        pred_non_negative += 1
                    
                    # ç»Ÿè®¡ä¸åŒç½®ä¿¡åº¦åŒºé—´çš„ç²¾ç¡®åº¦
                    if 0.50 <= probability < 0.55:
                        confidence_stats['0.50-0.55'][1] += 1
                        if target == 1:
                            confidence_stats['0.50-0.55'][0] += 1
                        if actual_return >= 0:
                            confidence_stats['0.50-0.55'][2] += 1
                    elif 0.55 <= probability < 0.58:
                        confidence_stats['0.55-0.58'][1] += 1
                        if target == 1:
                            confidence_stats['0.55-0.58'][0] += 1
                        if actual_return >= 0:
                            confidence_stats['0.55-0.58'][2] += 1
                    elif 0.58 <= probability < 0.60:
                        confidence_stats['0.58-0.60'][1] += 1
                        if target == 1:
                            confidence_stats['0.58-0.60'][0] += 1
                        if actual_return >= 0:
                            confidence_stats['0.58-0.60'][2] += 1
                    elif 0.60 <= probability < 0.70:
                        confidence_stats['0.60-0.70'][1] += 1
                        if target == 1:
                            confidence_stats['0.60-0.70'][0] += 1
                        if actual_return >= 0:
                            confidence_stats['0.60-0.70'][2] += 1
                    elif 0.70 <= probability <= 1.00:
                        confidence_stats['0.70-1.00'][1] += 1
                        if target == 1:
                            confidence_stats['0.70-1.00'][0] += 1
                        if actual_return >= 0:
                            confidence_stats['0.70-1.00'][2] += 1
                
                # ç»Ÿè®¡é¢„æµ‹æ­£ç¡®æ€§ï¼ˆç”¨äºæ˜¾ç¤ºå‡†ç¡®ç‡ï¼‰
                if prediction == target:
                    class_correct[target] += 1
    
    # è®¡ç®—AUC
    try:
        auc_score = roc_auc_score(all_targets, all_probabilities)
    except ValueError:
        # å¦‚æœæ‰€æœ‰æ ‡ç­¾éƒ½æ˜¯åŒä¸€ç±»ï¼ŒAUCæ— æ³•è®¡ç®—
        auc_score = 0.5  # éšæœºåˆ†ç±»å™¨çš„AUC
    
    # ğŸ”‘ æ ¸å¿ƒæ”¹è¿›ï¼šæŒ‰é¢„æµ‹æ¦‚ç‡æ’åºï¼Œè®¡ç®—Top N%æ ·æœ¬çš„æ”¶ç›Šç»Ÿè®¡
    # è¿™èƒ½çœŸå®åæ˜ æ¨¡å‹çš„æ’åºèƒ½åŠ›ï¼ˆé€‰è‚¡èƒ½åŠ›ï¼‰
    all_probabilities = np.array(all_probabilities)
    all_targets = np.array(all_targets)
    all_returns = np.array(all_returns)
    
    # æŒ‰é¢„æµ‹æ¦‚ç‡ä»é«˜åˆ°ä½æ’åº
    sorted_indices = np.argsort(all_probabilities)[::-1]  # é™åºæ’åˆ—
    
    # è®¡ç®—Top N%çš„ç»Ÿè®¡ï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ç™¾åˆ†æ¯”ï¼‰
    percent = DataConfig.TOP_PERCENT
    top_k = max(1, int(len(sorted_indices) * percent / 100))  # è‡³å°‘1ä¸ªæ ·æœ¬
    top_indices = sorted_indices[:top_k]
    
    top_returns = all_returns[top_indices]
    top_targets = all_targets[top_indices]
    
    # ç»Ÿè®¡ï¼šæ ·æœ¬æ•°ã€ç´¯è®¡æ”¶ç›Šã€å¹³å‡æ”¶ç›Š
    top_stats = {
        'count': top_k,
        'total_return': np.sum(top_returns),
        'avg_return': np.mean(top_returns),
    }
    
    return total, class_correct, class_total, pred_positive_correct, pred_positive_total, pred_non_negative, auc_score, confidence_stats, top_stats

def calculate_test_loss(model, eval_inputs, eval_targets, criterion, device, batch_size=DataConfig.EVAL_BATCH_SIZE):
    """
    è®¡ç®—æµ‹è¯•é›†æŸå¤±å€¼
    """
    model.eval()
    total_loss = 0
    num_samples = len(eval_inputs)
    num_batches = (num_samples + batch_size - 1) // batch_size
    
    with torch.no_grad():
        for i in range(num_batches):
            start_idx = i * batch_size
            end_idx = min((i + 1) * batch_size, num_samples)
            
            # æ‰¹é‡å¤„ç† (ä½¿ç”¨BF16ç²¾åº¦)
            batch_inputs = torch.tensor(eval_inputs[start_idx:end_idx], 
                                      dtype=torch.bfloat16).to(device)
            batch_targets = torch.tensor(eval_targets[start_idx:end_idx], 
                                       dtype=torch.bfloat16).to(device)
            
            # è®¡ç®—æŸå¤±
            batch_outputs = model(batch_inputs)
            batch_loss = criterion(batch_outputs, batch_targets)
            # BF16çš„losså¯ä»¥ç›´æ¥å–item
            total_loss += batch_loss.item()
    
    avg_loss = total_loss / num_batches
    return avg_loss

def print_sample_predictions(model, eval_inputs, eval_targets, device, num_samples=10, epoch=None):
    """
    éšæœºæŒ‘é€‰æ ·æœ¬å¹¶æ‰“å°æ¨¡å‹çš„è¾“å‡ºå€¼ï¼Œç”¨äºè§‚å¯Ÿé¢„æµ‹é›†ä¸­çš„é—®é¢˜
    """
    model.eval()
    
    # éšæœºé€‰æ‹©æ ·æœ¬ç´¢å¼•
    total_samples = len(eval_inputs)
    if num_samples > total_samples:
        num_samples = total_samples
    
    # ä½¿ç”¨å½“å‰epochä½œä¸ºéšæœºç§å­ï¼Œç¡®ä¿æ¯è½®é€‰æ‹©ä¸åŒçš„æ ·æœ¬
    if epoch is not None:
        np.random.seed(DataConfig.RANDOM_SEED + epoch)
    
    sample_indices = np.random.choice(total_samples, size=num_samples, replace=False)
    sample_indices = sorted(sample_indices)  # æ’åºä»¥ä¾¿è§‚å¯Ÿ
    
    print(f"  éšæœºæ ·æœ¬é¢„æµ‹è¯¦æƒ… (ç¬¬{epoch}è½®):")
    print(f"  {'æ ·æœ¬':<4} {'çœŸå®æ ‡ç­¾':<8} {'æ¨¡å‹è¾“å‡º':<12} {'é¢„æµ‹æ¦‚ç‡':<10} {'é¢„æµ‹æ ‡ç­¾':<8} {'é¢„æµ‹ç»“æœ':<8}")
    print(f"  {'-'*4} {'-'*8} {'-'*12} {'-'*10} {'-'*8} {'-'*8}")
    
    with torch.no_grad():
        for i, idx in enumerate(sample_indices):
            # è·å–å•ä¸ªæ ·æœ¬ (ä½¿ç”¨BF16ç²¾åº¦)
            sample_input = torch.tensor(eval_inputs[idx:idx+1], dtype=torch.bfloat16).to(device)
            true_label = eval_targets[idx]
            
            # æ¨¡å‹é¢„æµ‹
            model_output = model(sample_input)
            # BF16éœ€è¦å…ˆè½¬ä¸ºFP32å†è½¬pythonæ ‡é‡
            raw_output = model_output.float().cpu().item()
            probability = torch.sigmoid(model_output).float().cpu().item()
            predicted_label = 1 if probability > 0.5 else 0
            
            # åˆ¤æ–­é¢„æµ‹ç»“æœ
            if predicted_label == int(true_label):
                result = "âœ“æ­£ç¡®"
            else:
                if int(true_label) == 1 and predicted_label == 0:
                    result = "âœ—æ¼æ¶¨"
                elif int(true_label) == 0 and predicted_label == 1:
                    result = "âœ—è¯¯æ¶¨"
                else:
                    result = "âœ—é”™è¯¯"
            
            # æ ¼å¼åŒ–è¾“å‡º
            true_label_str = "ä¸Šæ¶¨" if int(true_label) == 1 else "ä¸ä¸Šæ¶¨"
            pred_label_str = "ä¸Šæ¶¨" if predicted_label == 1 else "ä¸ä¸Šæ¶¨"
            
            print(f"  {i+1:<4} {true_label_str:<8} {raw_output:<12.4f} {probability:<10.4f} {pred_label_str:<8} {result:<8}")
    
    print()  # ç©ºè¡Œ

# é¢„è®¡ç®—è®­ç»ƒæ•°æ®é›†å‡½æ•°
def precompute_training_dataset(train_stock_info, train_weights, 
                               batch_size, batches_per_epoch, seed=None):
    """
    é¢„è®¡ç®—æ¯è½®è®­ç»ƒæ‰€éœ€çš„è®­ç»ƒæ•°æ®é›†ï¼ˆä½¿ç”¨æ»šåŠ¨çª—å£æ ‡å‡†åŒ–ï¼‰
    è‡ªåŠ¨æ ¹æ®æ‰¹å¤§å°å’Œæ‰¹æ•°é‡è®¡ç®—éœ€è¦çš„æ ·æœ¬æ•°
    è¿”å›: (epoch_inputs, epoch_targets)
    """
    samples_per_epoch = batch_size * batches_per_epoch
    
    if seed is not None:
        # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
        np.random.seed(seed)
        random.seed(seed)
    
    # ç›´æ¥ç”Ÿæˆæ‰€æœ‰éœ€è¦çš„æ ·æœ¬ï¼ˆä½¿ç”¨æ»šåŠ¨çª—å£æ ‡å‡†åŒ–ï¼‰
    epoch_inputs, epoch_targets = generate_batch_samples_improved(
        train_stock_info, train_weights, samples_per_epoch)
    
    return np.array(epoch_inputs), np.array(epoch_targets)

# æ”¹è¿›çš„è®­ç»ƒå‡½æ•°
def train_model(model, train_stock_info, test_stock_info, train_weights, epochs=TrainingConfig.EPOCHS, 
               learning_rate=TrainingConfig.LEARNING_RATE, device=None, 
               batch_size=TrainingConfig.BATCH_SIZE, batches_per_epoch=TrainingConfig.BATCHES_PER_EPOCH):
    """
    ä½¿ç”¨é¢„è®¡ç®—è®­ç»ƒæ•°æ®é›†å’Œå›ºå®šè¯„ä¼°é›†çš„è®­ç»ƒå‡½æ•°ï¼ˆä½¿ç”¨æ»šåŠ¨çª—å£æ ‡å‡†åŒ–é¿å…æ•°æ®æ³„éœ²ï¼‰
    æé«˜è®­ç»ƒæ•ˆç‡ï¼Œç¡®ä¿è¯„ä¼°çš„ä¸€è‡´æ€§
    
    æ³¨æ„ï¼šæœ¬è®­ç»ƒå‡½æ•°ä½¿ç”¨ BF16 (bfloat16) ç²¾åº¦è¿›è¡Œè®­ç»ƒ
    - è®­ç»ƒé€Ÿåº¦æ¯”FP32å¿«çº¦2å€
    - å†…å­˜å ç”¨å‡åŠ
    - æ¨¡å‹ç²¾åº¦ä¸FP32ç›¸å½“
    """
    print("\n" + "="*60)
    print("è®­ç»ƒé…ç½®")
    print("="*60)
    print("è®­ç»ƒç²¾åº¦: BF16 (Brain Floating Point 16)")
    print("æ•°æ®æ ‡å‡†åŒ–: æ»šåŠ¨çª—å£æ ‡å‡†åŒ–ï¼ˆé¿å…æ•°æ®æ³„éœ²ï¼‰")
    print(f"æ•°æ®åˆ’åˆ†: æŒ‰æ—¶é—´åˆ’åˆ†ï¼Œæœ€è¿‘{DataConfig.TEST_DAYS}å¤©ä½œä¸ºæµ‹è¯•é›†")
    print("="*60 + "\n")
    # è®¾ç½®è®­ç»ƒéšæœºç§å­
    torch.manual_seed(DataConfig.RANDOM_SEED)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(DataConfig.RANDOM_SEED)
        torch.cuda.manual_seed_all(DataConfig.RANDOM_SEED)
    
    # åˆ›å»ºå›ºå®šçš„è¯„ä¼°æ•°æ®é›†ï¼ˆè®­ç»ƒå¼€å§‹å‰åˆ›å»ºä¸€æ¬¡ï¼Œä½¿ç”¨æ»šåŠ¨çª—å£æ ‡å‡†åŒ–ï¼‰
    eval_inputs, eval_targets, eval_cumulative_returns = create_fixed_evaluation_dataset(test_stock_info)
    
    # ä½¿ç”¨åŠ¨æ€åŠ æƒBCEæŸå¤±å‡½æ•°ï¼Œæ ¹æ®æ¯è½®è®­ç»ƒæ•°æ®çš„æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹åŠ¨æ€è°ƒæ•´æƒé‡
    # æ­£æ ·æœ¬æƒé‡å›ºå®šä¸º4.0ï¼Œè´Ÿæ ·æœ¬æƒé‡åŠ¨æ€è°ƒæ•´ï¼ˆ0.5~1.0ï¼‰
    criterion = DynamicWeightedBCE(pos_weight=4.0)
    
    # åˆ›å»ºæµ‹è¯•é›†ä¸“ç”¨çš„æŸå¤±å‡½æ•°ï¼ˆä½¿ç”¨æ ‡å‡†BCEï¼Œæ­£è´Ÿæ ·æœ¬æƒé‡éƒ½ä¸º1.0ï¼Œä¿è¯å¯æ¯”æ€§ï¼‰
    eval_criterion = DynamicWeightedBCE(pos_weight=1.0)
    # ä¸è°ƒç”¨update_weightsï¼Œä¿æŒåˆå§‹å€¼: pos_weight=1.0, neg_weight=1.0
    
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=TrainingConfig.WEIGHT_DECAY)
    
    # åˆ›å»ºé¢„çƒ­è°ƒåº¦å™¨
    warmup_scheduler = WarmupScheduler(
        optimizer, 
        warmup_epochs=TrainingConfig.WARMUP_EPOCHS,
        target_lr=learning_rate,
        start_lr=TrainingConfig.WARMUP_START_LR
    )
    
    # åˆ›å»ºä¸»è°ƒåº¦å™¨
    # æ³¨æ„ï¼šè™½ç„¶warmup_schedulerå·²ç»å°†optimizerçš„å­¦ä¹ ç‡è®¾ç½®ä¸ºstart_lrï¼Œ
    # ä½†ä¸»è°ƒåº¦å™¨åº”è¯¥åŸºäºtarget_lræ¥å·¥ä½œã€‚
    # æˆ‘ä»¬åœ¨åˆ›å»ºä¸»è°ƒåº¦å™¨å‰å…ˆä¸´æ—¶è®¾ç½®ä¸ºtarget_lrï¼Œè¿™æ ·ä¸»è°ƒåº¦å™¨å°±ä¼šä»¥æ­£ç¡®çš„å­¦ä¹ ç‡ä¸ºåŸºå‡†
    for param_group in optimizer.param_groups:
        param_group['lr'] = learning_rate
    
    # æ ¹æ®é…ç½®é€‰æ‹©ä¸»è°ƒåº¦å™¨
    if TrainingConfig.USE_COSINE_ANNEALING:
        # ä¿®å¤ï¼šä½¿ç”¨æ€»è½®æ•°-é¢„çƒ­è½®æ•°ä½œä¸ºT_maxï¼Œç¡®ä¿ä½™å¼¦é€€ç«è¦†ç›–æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹
        # é¿å…åœ¨è®­ç»ƒåæœŸå­¦ä¹ ç‡å†æ¬¡ä¸Šå‡
        total_main_epochs = epochs - TrainingConfig.WARMUP_EPOCHS
        main_scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=total_main_epochs,  # ä½¿ç”¨å®é™…çš„ä¸»è®­ç»ƒè½®æ•°
            eta_min=TrainingConfig.COSINE_ETA_MIN
        )
        scheduler_type = f"ä½™å¼¦é€€ç«(å‘¨æœŸ={total_main_epochs}è½®)"
    else:
        main_scheduler = optim.lr_scheduler.StepLR(
            optimizer, 
            step_size=TrainingConfig.SCHEDULER_STEP_SIZE, 
            gamma=TrainingConfig.SCHEDULER_GAMMA
        )
        scheduler_type = "é˜¶æ¢¯è¡°å‡"
    
    # åˆ›å»ºä¸»è°ƒåº¦å™¨åï¼Œéœ€è¦å°†å­¦ä¹ ç‡é‡æ–°è®¾ç½®å›start_lrï¼Œå› ä¸ºè®­ç»ƒä»é¢„çƒ­å¼€å§‹
    for param_group in optimizer.param_groups:
        param_group['lr'] = TrainingConfig.WARMUP_START_LR
    
    print(f"å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥: {scheduler_type}")
    
    best_loss = float('inf')  # ä½¿ç”¨æµ‹è¯•é›†lossä½œä¸ºä¿å­˜æ ‡å‡†ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
    best_model_state = None  # ç¼“å­˜æœ€ä½³æ¨¡å‹çŠ¶æ€ï¼ˆå†…å­˜ä¸­ï¼‰
    best_epoch = 0  # è®°å½•æœ€ä½³æ¨¡å‹æ‰€åœ¨è½®æ¬¡
    
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        
        # è®­ç»ƒé˜¶æ®µ - æ›´æ–°å­¦ä¹ ç‡
        if warmup_scheduler.is_warmup_phase():
            # é¢„çƒ­é˜¶æ®µï¼šä½¿ç”¨é¢„çƒ­è°ƒåº¦å™¨
            current_lr = warmup_scheduler.step(epoch)
            lr_status = f"é¢„çƒ­é˜¶æ®µ ({epoch + 1}/{TrainingConfig.WARMUP_EPOCHS})"
        else:
            # é¢„çƒ­ç»“æŸåï¼šä½¿ç”¨ä¸»è°ƒåº¦å™¨è·å–å½“å‰å­¦ä¹ ç‡
            current_lr = main_scheduler.get_last_lr()[0]
            lr_status = "æ­£å¸¸è®­ç»ƒ"
        
        print(f'Epoch {epoch + 1}/{epochs}, LR: {current_lr:.6f} ({lr_status})')
        
        # é¢„è®¡ç®—å½“å‰è½®æ¬¡çš„è®­ç»ƒæ•°æ®ï¼ˆä½¿ç”¨æ»šåŠ¨çª—å£æ ‡å‡†åŒ–ï¼‰
        epoch_seed = DataConfig.RANDOM_SEED + epoch  # æ¯è½®ä½¿ç”¨ä¸åŒçš„ç§å­ç¡®ä¿æ•°æ®å¤šæ ·æ€§
        epoch_inputs, epoch_targets = precompute_training_dataset(
            train_stock_info, train_weights, batch_size, batches_per_epoch, epoch_seed)
        
        # æ³¨æ„ï¼šåŠ¨æ€æƒé‡æ›´æ–°å·²ç§»è‡³æ¯ä¸ªBatchå†…éƒ¨ï¼Œç¡®ä¿æ¯æ¬¡å‚æ•°æ›´æ–°æ—¶éƒ½åŸºäºå½“å‰Batchçš„æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹è¿›è¡Œå¹³è¡¡
        
        # æ‰“å°æœ¬è½®æ ‡ç­¾åˆ†å¸ƒä¿¡æ¯ï¼ˆäºŒåˆ†ç±»ï¼š1.0/0.0ï¼‰
        count_positive = np.sum(epoch_targets >= 0.5)  # æ­£æ ·æœ¬ï¼ˆæ¶¨å¹…â‰¥5%ï¼‰
        count_negative = np.sum(epoch_targets < 0.5)   # è´Ÿæ ·æœ¬ï¼ˆæ¶¨å¹…<5%ï¼‰
        total_count = len(epoch_targets)
        
        print(f'  æ ‡ç­¾åˆ†å¸ƒ: ä¸Šæ¶¨(â‰¥5%)={count_positive}({count_positive/total_count:.1%}), ä¸ä¸Šæ¶¨(<5%)={count_negative}({count_negative/total_count:.1%})')
        print(f'  åŠ¨æ€æƒé‡: æ¯Batchç‹¬ç«‹è®¡ç®—ï¼ˆæ­£æ ·æœ¬å›ºå®š={criterion.pos_weight.item():.1f}ï¼Œè´Ÿæ ·æœ¬æŒ‰æ¯”ä¾‹åŠ¨æ€è°ƒæ•´ï¼‰')
        
        # æ˜¾ç¤ºé¢„çƒ­è¿›åº¦å’Œè°ƒåº¦å™¨ä¿¡æ¯
        if warmup_scheduler.is_warmup_phase():
            warmup_progress = (epoch + 1) / TrainingConfig.WARMUP_EPOCHS * 100
            print(f'  é¢„çƒ­è¿›åº¦: {warmup_progress:.1f}% (ç¬¬{epoch + 1}è½®/å…±{TrainingConfig.WARMUP_EPOCHS}è½®)')
            print(f'  å­¦ä¹ ç‡å˜åŒ–: {TrainingConfig.WARMUP_START_LR:.2e} â†’ {current_lr:.2e} â†’ {learning_rate:.2e}(ç›®æ ‡)')
        else:
            # é¢„çƒ­ç»“æŸåæ˜¾ç¤ºå½“å‰è°ƒåº¦å™¨çŠ¶æ€
            if TrainingConfig.USE_COSINE_ANNEALING:
                # è®¡ç®—ä½™å¼¦é€€ç«çš„ç†è®ºå­¦ä¹ ç‡
                import math
                # ä¿®å¤ï¼šä½¿ç”¨å®é™…çš„ä¸»è®­ç»ƒè½®æ•°è®¡ç®—è¿›åº¦
                total_main_epochs = epochs - TrainingConfig.WARMUP_EPOCHS
                current_main_epoch = epoch - TrainingConfig.WARMUP_EPOCHS
                progress = current_main_epoch / total_main_epochs
                theoretical_lr = TrainingConfig.COSINE_ETA_MIN + (learning_rate - TrainingConfig.COSINE_ETA_MIN) * \
                               (1 + math.cos(math.pi * progress)) / 2
                print(f'  ä½™å¼¦é€€ç«è¿›åº¦: {progress*100:.1f}% (ç¬¬{current_main_epoch+1}è½®/å…±{total_main_epochs}è½®), ç†è®ºå­¦ä¹ ç‡: {theoretical_lr:.2e}')
            else:
                print(f'  é˜¶æ¢¯è¡°å‡: æ¯{TrainingConfig.SCHEDULER_STEP_SIZE}è½®è¡°å‡{TrainingConfig.SCHEDULER_GAMMA}å€')
        
        # å°†é¢„è®¡ç®—çš„æ•°æ®è½¬æ¢ä¸ºtensorå¹¶ç§»åˆ°è®¾å¤‡ä¸Š (ä½¿ç”¨BF16ç²¾åº¦)
        epoch_inputs_tensor = torch.tensor(epoch_inputs, dtype=torch.bfloat16).to(device)
        epoch_targets_tensor = torch.tensor(epoch_targets, dtype=torch.bfloat16).to(device)
        
        # è®­ç»ƒå¾ªç¯ï¼šä½¿ç”¨é¢„è®¡ç®—çš„æ•°æ®
        for step in range(batches_per_epoch):
            start_idx = step * batch_size
            end_idx = min((step + 1) * batch_size, len(epoch_inputs_tensor))
            
            # ä»é¢„è®¡ç®—çš„æ•°æ®ä¸­å–ä¸€ä¸ªbatch
            batch_inputs = epoch_inputs_tensor[start_idx:end_idx]
            batch_targets = epoch_targets_tensor[start_idx:end_idx]
            
            # ğŸ”‘ æ¯ä¸ªBatchéƒ½æ›´æ–°åŠ¨æ€æƒé‡ï¼Œç¡®ä¿æ¯æ¬¡å‚æ•°æ›´æ–°æ—¶æ­£è´Ÿæ ·æœ¬è´¡çŒ®å¹³è¡¡
            criterion.update_weights(batch_targets)
            
            optimizer.zero_grad()
            output = model(batch_inputs)
            loss = criterion(output, batch_targets)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=TrainingConfig.GRADIENT_CLIP_NORM)
            optimizer.step()
            
            total_loss += loss.item()
            
            # å®æ—¶æ›´æ–°è¿›åº¦æ˜¾ç¤º
            progress = (step + 1) / batches_per_epoch * 100
            avg_loss = total_loss / (step + 1)
            print(f'\r  è®­ç»ƒè¿›åº¦: {progress:.1f}% ({step + 1}/{batches_per_epoch}), å¹³å‡æŸå¤±: {avg_loss:.4f}', end='', flush=True)
        
        print()  # æ¢è¡Œ
        print()  # ç©ºè¡Œ
        
        # æ¸…ç†é¢„è®¡ç®—çš„æ•°æ®ä»¥é‡Šæ”¾å†…å­˜
        del epoch_inputs_tensor, epoch_targets_tensor
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # æ›´æ–°å­¦ä¹ ç‡
        # æ³¨æ„ï¼šé¢„çƒ­é˜¶æ®µçš„å­¦ä¹ ç‡å·²ç»åœ¨epochå¼€å§‹æ—¶ç”±warmup_scheduler.step()æ›´æ–°
        # åªæœ‰é¢„çƒ­ç»“æŸåæ‰ä½¿ç”¨ä¸»è°ƒåº¦å™¨
        if not warmup_scheduler.is_warmup_phase():
            main_scheduler.step()  # æ›´æ–°ä¸»è°ƒåº¦å™¨ï¼ˆä½™å¼¦é€€ç«æˆ–é˜¶æ¢¯è¡°å‡ï¼‰
        
        # å›ºå®šè¯„ä¼°é›†è¯„ä¼°
        total, class_correct, class_total, pred_positive_correct, pred_positive_total, pred_non_negative, auc_score, confidence_stats, top_stats = evaluate_model_batch(
            model, eval_inputs, eval_targets, eval_cumulative_returns, device, batch_size=DataConfig.EVAL_BATCH_SIZE
        )
        
        # è®¡ç®—æµ‹è¯•é›†æŸå¤±ï¼ˆä½¿ç”¨å›ºå®šæƒé‡çš„eval_criterionï¼Œä¿è¯å¯æ¯”æ€§ï¼‰
        test_loss = calculate_test_loss(model, eval_inputs, eval_targets, eval_criterion, device, batch_size=DataConfig.EVAL_BATCH_SIZE)
        
        # éšæœºæŒ‘é€‰5ç»„æ ·æœ¬æ‰“å°æ¨¡å‹è¾“å‡ºå€¼
        print_sample_predictions(model, eval_inputs, eval_targets, device, num_samples=5, epoch=epoch+1)
        
        # æ‰“å°è¯¦ç»†ç»“æœ
        class_names = ['ä¸ä¸Šæ¶¨', 'ä¸Šæ¶¨']
        for i in range(2):
            if class_total[i] > 0:
                acc = class_correct[i] / class_total[i]
                print(f'  {class_names[i]}: {class_correct[i]}/{class_total[i]} = {acc:.3f}')
            else:
                print(f'  {class_names[i]}: 0/0 = 0.000 (æ— æ ·æœ¬)')
        
        # è®¡ç®—ä¸Šæ¶¨å‡†ç¡®ç‡ï¼ˆé¢„æµ‹ä¸Šæ¶¨åçœŸä¸Šæ¶¨çš„æ¦‚ç‡ï¼‰
        if pred_positive_total > 0:
            precision = pred_positive_correct / pred_positive_total
            non_negative_rate = pred_non_negative / pred_positive_total
            print(f'  ä¸Šæ¶¨å‡†ç¡®ç‡: {pred_positive_correct}/{pred_positive_total} = {precision:.3f} å‡†ç¡®ç‡: {pred_non_negative}/{pred_positive_total} = {non_negative_rate:.3f}')
        else:
            print(f'  ä¸Šæ¶¨å‡†ç¡®ç‡: 0/0 = 0.000 (æ— é¢„æµ‹ä¸Šæ¶¨)')
        
        # æ‰“å°ç½®ä¿¡åº¦åŒºé—´çš„ç²¾ç¡®åº¦ç»Ÿè®¡
        print(f'  ç½®ä¿¡åº¦åŒºé—´ç²¾ç¡®åº¦:')
        for interval in ['0.50-0.55', '0.55-0.58', '0.58-0.60', '0.60-0.70', '0.70-1.00']:
            correct, total_pred, non_negative = confidence_stats[interval]
            if total_pred > 0:
                precision = correct / total_pred
                non_negative_rate = non_negative / total_pred
                print(f'    {interval}: ä¸Šæ¶¨å‡†ç¡®={correct}/{total_pred}={precision:.3f}, éè´Ÿå‡†ç¡®={non_negative}/{total_pred}={non_negative_rate:.3f}')
            else:
                print(f'    {interval}: æ— é¢„æµ‹')
        
        overall_acc = sum(class_correct) / sum(class_total) if sum(class_total) > 0 else 0
        avg_loss = total_loss / batches_per_epoch
        
        print(f'  æ€»ä½“å‡†ç¡®ç‡: {overall_acc:.3f}')
        print(f'  Top{DataConfig.TOP_PERCENT}%æ”¶ç›Š: æ ·æœ¬æ•°={top_stats["count"]}, å¹³å‡={top_stats["avg_return"]*100:+.2f}%, ç´¯è®¡={top_stats["total_return"]*100:+.2f}%')
        print(f'  AUCå¾—åˆ†: {auc_score:.4f}')
        print(f'  è®­ç»ƒé›†æŸå¤±: {avg_loss:.4f}, æµ‹è¯•é›†æŸå¤±: {test_loss:.4f}')
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆä½¿ç”¨æµ‹è¯•é›†lossä½œä¸ºä¸»è¦æ ‡å‡†ï¼ŒåŒæ—¶ç›‘æ§AUCï¼‰
        MIN_AUC = DataConfig.MIN_AUC
        
        # åˆ¤æ–­æ˜¯å¦ä¿å­˜æ¨¡å‹
        should_save = False
        save_reason = ""
        
        if auc_score < MIN_AUC:
            print(f'  âš  AUCè¿‡ä½({auc_score:.4f}<{MIN_AUC})ï¼Œæ¨¡å‹åˆ†ç±»èƒ½åŠ›ä¸è¶³ï¼Œæš‚ä¸æ›´æ–°')
        elif test_loss < best_loss:
            should_save = True
            save_reason = f'æµ‹è¯•é›†Lossé™ä½: {best_loss:.4f} â†’ {test_loss:.4f}'
        
        if should_save:
            best_loss = test_loss
            best_epoch = epoch + 1
            # ç¼“å­˜æ¨¡å‹çŠ¶æ€åˆ°å†…å­˜ï¼ˆæ·±æ‹·è´ï¼‰ï¼Œä¸ç«‹å³å†™å…¥ç£ç›˜
            import copy
            best_model_state = copy.deepcopy(model.state_dict())
            print(f'  âœ“ å‘ç°æ›´å¥½çš„æ¨¡å‹ï¼{save_reason}ï¼ˆå·²ç¼“å­˜åˆ°å†…å­˜ï¼‰')
            print(f'    è¯¦æƒ…: AUC={auc_score:.4f}, Top{DataConfig.TOP_PERCENT}%æ”¶ç›Š: å¹³å‡={top_stats["avg_return"]*100:+.2f}%, ç´¯è®¡={top_stats["total_return"]*100:+.2f}%')
        
        print("-" * 50)
    
    # è®­ç»ƒç»“æŸåï¼Œå°†æœ€ä½³æ¨¡å‹ä¿å­˜åˆ°ç£ç›˜
    if best_model_state is not None:
        print("\n" + "=" * 50)
        print(f"è®­ç»ƒå®Œæˆï¼æ­£åœ¨ä¿å­˜æœ€ä½³æ¨¡å‹...")
        print(f"æœ€ä½³æ¨¡å‹æ¥è‡ªç¬¬ {best_epoch} è½®ï¼Œæµ‹è¯•é›†Loss: {best_loss:.4f}")
        torch.save(best_model_state, ModelSaveConfig.get_best_model_path())
        print(f"âœ“ æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {ModelSaveConfig.get_best_model_path()}")
        print("=" * 50)
    else:
        print("\n" + "=" * 50)
        print("âš  è­¦å‘Šï¼šæœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æœ€ä½³æ¨¡å‹ï¼ˆAUCè¦æ±‚æœªè¾¾æ ‡ï¼‰")
        print("=" * 50)

if __name__ == "__main__":
    # è®¾ç½®å·¥ä½œç›®å½•
    os.chdir(os.path.dirname(os.path.abspath(__file__)))
    
    # æ‰“å°é…ç½®æ‘˜è¦
    print_config_summary()
    
    # è·å–è®¾å¤‡ä¿¡æ¯
    device = DeviceConfig.print_device_info()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(DataConfig.OUTPUT_DIR, exist_ok=True)
    
    # ä½¿ç”¨æ”¹è¿›çš„æ•°æ®åŠ è½½å‡½æ•°ï¼ˆæŒ‰æ—¶é—´åˆ’åˆ†ï¼Œé¿å…æ•°æ®æ³„éœ²ï¼‰
    print("æ­£åœ¨åŠ è½½å’Œé¢„å¤„ç†æ•°æ®...")
    train_stock_info, test_stock_info = load_and_preprocess_data()

    # è®¡ç®—è‚¡ç¥¨é€‰æ‹©æƒé‡
    train_weights = calculate_stock_weights(train_stock_info)
    test_weights = calculate_stock_weights(test_stock_info)
    
    # æ‰“å°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "="*60)
    print("æ•°æ®é›†åˆ’åˆ†ç»Ÿè®¡")
    print("="*60)
    
    train_lengths = [info['data_length'] for info in train_stock_info]
    test_lengths = [info['data_length'] for info in test_stock_info]
    
    print(f"è®­ç»ƒé›†:")
    print(f"  è‚¡ç¥¨æ•°é‡: {len(train_stock_info)}")
    print(f"  æ•°æ®é•¿åº¦: æœ€å°={min(train_lengths)}, æœ€å¤§={max(train_lengths)}, å¹³å‡={np.mean(train_lengths):.1f}")
    print(f"  é‡‡æ ·æƒé‡: {min(train_weights):.3f} - {max(train_weights):.3f}")
    
    print(f"\næµ‹è¯•é›†:")
    print(f"  è‚¡ç¥¨æ•°é‡: {len(test_stock_info)}")
    print(f"  æ•°æ®é•¿åº¦: æœ€å°={min(test_lengths)}, æœ€å¤§={max(test_lengths)}, å¹³å‡={np.mean(test_lengths):.1f}")
    print(f"  æ—¶é—´èŒƒå›´: æ¯åªè‚¡ç¥¨çš„æœ€è¿‘ {DataConfig.TEST_DAYS} å¤©")
    
    print(f"\nå‰3åªè‚¡ç¥¨ç¤ºä¾‹:")
    for i in range(min(3, len(train_stock_info))):
        train_info = train_stock_info[i]
        print(f"  {train_info['file_name']}: è®­ç»ƒé›†é•¿åº¦={train_info['data_length']}, æƒé‡={train_weights[i]:.3f}")
    
    print("="*60)

    print("æ­£åœ¨åˆ›å»º Transformer æ¨¡å‹ (BF16ç²¾åº¦)...")
    model = EnhancedStockTransformer(
        input_dim=ModelConfig.INPUT_DIM, 
        d_model=ModelConfig.D_MODEL, 
        nhead=ModelConfig.NHEAD, 
        num_layers=ModelConfig.NUM_LAYERS, 
        output_dim=ModelConfig.OUTPUT_DIM,
        max_seq_len=ModelConfig.MAX_SEQ_LEN
    ).to(device)
    
    # å°†æ¨¡å‹å‚æ•°è½¬æ¢ä¸ºBF16ç²¾åº¦
    model = model.to(dtype=torch.bfloat16)
    
    # æ‰“å°æ¨¡å‹å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ¨¡å‹æ€»å‚æ•°æ•°: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°æ•°: {trainable_params:,}")

    print("å¼€å§‹è®­ç»ƒ...")
    # ä½¿ç”¨å¸¦å›ºå®šè¯„ä¼°é›†çš„è®­ç»ƒå‡½æ•°ï¼ˆä½¿ç”¨æ»šåŠ¨çª—å£æ ‡å‡†åŒ–ï¼‰
    train_model(model, train_stock_info, test_stock_info, train_weights, device=device)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆè®­ç»ƒç»“æŸæ—¶çš„çŠ¶æ€ï¼‰
    final_model_path = ModelSaveConfig.get_final_model_path(ModelConfig.D_MODEL)
    torch.save(model.state_dict(), final_model_path)
    print(f"\næœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_path}")

# ==================== ç»Ÿä¸€é¢„æµ‹å‡½æ•° ====================
def normalize_data_for_prediction(data):
    """
    ç»Ÿä¸€çš„æ•°æ®å½’ä¸€åŒ–å‡½æ•°ï¼ˆæ»šåŠ¨çª—å£æ ‡å‡†åŒ–ï¼‰
    ç”¨äºæ‰€æœ‰é¢„æµ‹åœºæ™¯ï¼Œç¡®ä¿ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´
    
    Args:
        data: numpy array, shape [seq_len, 5] (OHLCV)
        
    Returns:
        normalized_data: numpy array, shape [seq_len-1, 5] æˆ– Noneï¼ˆå¦‚æœæ•°æ®æ— æ•ˆï¼‰
    """
    if len(data) < 2:
        return None
    
    normalized_data = np.zeros_like(data, dtype=np.float64)
    
    # æ»šåŠ¨çª—å£æ ‡å‡†åŒ–ï¼šæ¯å¤©ç›¸å¯¹äºå‰ä¸€å¤©çš„æ¶¨è·Œå¹…
    for i in range(1, len(data)):
        yesterday_close = data[i-1, 3]  # å‰ä¸€å¤©çš„æ”¶ç›˜ä»·
        yesterday_volume = data[i-1, 4]  # å‰ä¸€å¤©çš„æˆäº¤é‡
        
        if yesterday_close == 0 or yesterday_volume == 0:
            return None  # æ•°æ®å¼‚å¸¸
        
        # ä»·æ ¼ç‰¹å¾ï¼šç›¸å¯¹äºå‰ä¸€å¤©æ”¶ç›˜ä»·çš„æ¶¨è·Œå¹…
        normalized_data[i, :4] = (data[i, :4] - yesterday_close) / yesterday_close
        # æˆäº¤é‡ç‰¹å¾ï¼šç›¸å¯¹äºå‰ä¸€å¤©æˆäº¤é‡çš„å˜åŒ–æ¯”ä¾‹
        normalized_data[i, 4] = (data[i, 4] - yesterday_volume) / yesterday_volume
    
    # åªè¿”å›æ ‡å‡†åŒ–åçš„æ•°æ®ï¼ˆå»æ‰ç¬¬0å¤©åŸºå‡†æ•°æ®ï¼‰
    return normalized_data[1:]

def predict_single_stock(model_path, stock_data, device=None):
    """
    ç»Ÿä¸€çš„å•è‚¡ç¥¨é¢„æµ‹å‡½æ•°
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        stock_data: numpy array, shape [seq_len, 5] (OHLCV)ï¼Œè‡³å°‘éœ€è¦CONTEXT_LENGTH+1å¤©æ•°æ®
        device: è®¡ç®—è®¾å¤‡
        
    Returns:
        probability: float, é¢„æµ‹æ¦‚ç‡ [0, 1]ï¼Œå¦‚æœé¢„æµ‹å¤±è´¥è¿”å›None
    """
    if device is None:
        device = DeviceConfig.get_device()
    
    # æ£€æŸ¥æ•°æ®é•¿åº¦
    if len(stock_data) < DataConfig.CONTEXT_LENGTH + 1:
        return None
    
    # å–æœ€æ–°æ•°æ®
    recent_data = stock_data[-(DataConfig.CONTEXT_LENGTH + 1):]
    
    # å½’ä¸€åŒ–
    normalized_data = normalize_data_for_prediction(recent_data)
    if normalized_data is None:
        return None
    
    # åŠ è½½æ¨¡å‹
    try:
        model = EnhancedStockTransformer(
            input_dim=ModelConfig.INPUT_DIM,
            d_model=ModelConfig.D_MODEL,
            nhead=ModelConfig.NHEAD,
            num_layers=ModelConfig.NUM_LAYERS,
            output_dim=ModelConfig.OUTPUT_DIM,
            max_seq_len=ModelConfig.MAX_SEQ_LEN
        ).to(device)
        
        model = model.to(dtype=torch.bfloat16)
        model.load_state_dict(torch.load(model_path, map_location=device))
        model.eval()
    except Exception as e:
        print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None
    
    # é¢„æµ‹
    try:
        input_tensor = torch.tensor(normalized_data, dtype=torch.bfloat16).unsqueeze(0).to(device)
        
        with torch.no_grad():
            output = model(input_tensor)
            probability = torch.sigmoid(output).float().cpu().item()
        
        return probability
    except Exception as e:
        print(f"é¢„æµ‹å¤±è´¥: {e}")
        return None

def predict_multiple_stocks(model_path, stock_files_data, device=None):
    """
    ç»Ÿä¸€çš„å¤šè‚¡ç¥¨é¢„æµ‹å‡½æ•°
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        stock_files_data: dict, {æ–‡ä»¶å: numpy_array}
        device: è®¡ç®—è®¾å¤‡
        
    Returns:
        predictions: list of (filename, probability)
    """
    if device is None:
        device = DeviceConfig.get_device()
    
    predictions = []
    
    # åŠ è½½æ¨¡å‹ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰
    try:
        model = EnhancedStockTransformer(
            input_dim=ModelConfig.INPUT_DIM,
            d_model=ModelConfig.D_MODEL,
            nhead=ModelConfig.NHEAD,
            num_layers=ModelConfig.NUM_LAYERS,
            output_dim=ModelConfig.OUTPUT_DIM,
            max_seq_len=ModelConfig.MAX_SEQ_LEN
        ).to(device)
        
        model = model.to(dtype=torch.bfloat16)
        model.load_state_dict(torch.load(model_path, map_location=device))
        model.eval()
    except Exception as e:
        print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return predictions
    
    # æ‰¹é‡é¢„æµ‹
    with torch.no_grad():
        for filename, stock_data in stock_files_data.items():
            # æ£€æŸ¥æ•°æ®é•¿åº¦
            if len(stock_data) < DataConfig.CONTEXT_LENGTH + 1:
                continue
            
            # å–æœ€æ–°æ•°æ®å¹¶å½’ä¸€åŒ–
            recent_data = stock_data[-(DataConfig.CONTEXT_LENGTH + 1):]
            normalized_data = normalize_data_for_prediction(recent_data)
            if normalized_data is None:
                continue
            
            try:
                # é¢„æµ‹
                input_tensor = torch.tensor(normalized_data, dtype=torch.bfloat16).unsqueeze(0).to(device)
                output = model(input_tensor)
                probability = torch.sigmoid(output).float().cpu().item()
                
                predictions.append((filename, probability))
            except Exception as e:
                print(f"{filename} é¢„æµ‹å¤±è´¥: {e}")
                continue
    
    return predictions