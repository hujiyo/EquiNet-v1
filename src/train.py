'''
è®­ç»ƒè„šæœ¬

è¯„åˆ†åˆ¶åº¦ï¼ˆä»¥ä»£ç å®ç°ä¸ºå‡†ï¼‰ï¼š
æä¾›é¢„æµ‹æœºä¼šï¼Œé¢„æµ‹æ­£ç¡®åŠ 1åˆ†
é¢„æµ‹é”™è¯¯åˆ™æŒ‰ä¸‹é¢ç­–ç•¥å¤„ç†ï¼š
1.å‡é˜³æ€§ï¼ˆé¢„æµ‹ä¸Šæ¶¨ä½†å®é™…ä¸ä¸Šæ¶¨ï¼‰ï¼š-1åˆ† 
2.å‡é˜´æ€§ï¼ˆé¢„æµ‹ä¸ä¸Šæ¶¨ä½†å®é™…ä¸Šæ¶¨ï¼‰ï¼š-0.5åˆ† 
3.å…¶ä½™æƒ…å†µä¸åŠ åˆ†ä¹Ÿä¸æ‰£åˆ†ã€‚
'''

import os,torch,torch.nn as nn,torch.optim as optim,pandas as pd,numpy as np
import random
import math
import torch.nn.functional as F
from sklearn.metrics import roc_auc_score
from config import (ModelConfig, TrainingConfig, DataConfig, 
                   EvaluationConfig, DeviceConfig, ModelSaveConfig,
                   print_config_summary)

# å­¦ä¹ ç‡é¢„çƒ­è°ƒåº¦å™¨
class WarmupScheduler:
    """
    å­¦ä¹ ç‡é¢„çƒ­è°ƒåº¦å™¨
    åœ¨å‰å‡ è½®è®­ç»ƒä¸­ï¼Œå­¦ä¹ ç‡ä»å¾ˆå°çš„å€¼é€æ­¥å¢åŠ åˆ°ç›®æ ‡å­¦ä¹ ç‡
    è¿™æœ‰åŠ©äºæ¨¡å‹åœ¨è®­ç»ƒåˆæœŸæ›´ç¨³å®šåœ°æ”¶æ•›
    """
    def __init__(self, optimizer, warmup_epochs, target_lr, start_lr=None):
        """
        Args:
            optimizer: PyTorchä¼˜åŒ–å™¨
            warmup_epochs: é¢„çƒ­è½®æ•°
            target_lr: ç›®æ ‡å­¦ä¹ ç‡ï¼ˆé¢„çƒ­ç»“æŸåçš„å­¦ä¹ ç‡ï¼‰
            start_lr: é¢„çƒ­èµ·å§‹å­¦ä¹ ç‡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨target_lrçš„1/100
        """
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.target_lr = target_lr
        self.start_lr = start_lr if start_lr is not None else target_lr / 100
        self.current_epoch = 0
        
        # è®¾ç½®åˆå§‹å­¦ä¹ ç‡ä¸ºé¢„çƒ­èµ·å§‹å­¦ä¹ ç‡
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = self.start_lr
    
    def step(self, epoch=None):
        """
        æ›´æ–°å­¦ä¹ ç‡
        Args:
            epoch: å½“å‰è½®æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å†…éƒ¨è®¡æ•°å™¨
        """
        if epoch is not None:
            self.current_epoch = epoch
        else:
            self.current_epoch += 1
        
        if self.current_epoch < self.warmup_epochs:
            # é¢„çƒ­é˜¶æ®µï¼šçº¿æ€§å¢åŠ å­¦ä¹ ç‡
            lr = self.start_lr + (self.target_lr - self.start_lr) * ((self.current_epoch + 1) / self.warmup_epochs)
        else:
            # é¢„çƒ­ç»“æŸåä¿æŒç›®æ ‡å­¦ä¹ ç‡
            lr = self.target_lr
        
        # æ›´æ–°ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
        
        return lr
    
    def get_last_lr(self):
        """è·å–å½“å‰å­¦ä¹ ç‡ï¼ˆå…¼å®¹PyTorchè°ƒåº¦å™¨æ¥å£ï¼‰"""
        return [param_group['lr'] for param_group in self.optimizer.param_groups]
    
    def is_warmup_phase(self):
        """åˆ¤æ–­æ˜¯å¦è¿˜åœ¨é¢„çƒ­é˜¶æ®µ"""
        return self.current_epoch < self.warmup_epochs

# åŠ¨æ€åŠ æƒBCEæŸå¤±å‡½æ•°å®ç°
class DynamicWeightedBCE(nn.Module):
    """
    åŠ¨æ€åŠ æƒBCEæŸå¤±å‡½æ•°ï¼Œæ ¹æ®æ¯è½®è®­ç»ƒæ•°æ®çš„æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹åŠ¨æ€è°ƒæ•´æƒé‡
    ä½¿ç”¨æ ‡å‡†çš„ç±»åˆ«ä¸å¹³è¡¡å¤„ç†å…¬å¼ï¼šweight = total_samples / (num_classes * class_count)
    """
    def __init__(self, reduction='mean'):
        super(DynamicWeightedBCE, self).__init__()
        self.reduction = reduction
        
        # åŠ¨æ€æƒé‡ï¼Œä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°
        self.register_buffer('positive_weight', torch.tensor(1.0))
        self.register_buffer('negative_weight', torch.tensor(1.0))
        
    def update_weights(self, targets):
        """
        æ ¹æ®å½“å‰æ‰¹æ¬¡çš„ç›®æ ‡æ ‡ç­¾æ›´æ–°æƒé‡
        ä½¿ç”¨æ ‡å‡†çš„ç±»åˆ«ä¸å¹³è¡¡å¤„ç†å…¬å¼ï¼šweight = total_samples / (num_classes * class_count)
        targets: [batch_size] çœŸå®æ ‡ç­¾ (0=ä¸ä¸Šæ¶¨, 1=ä¸Šæ¶¨)
        """
        if isinstance(targets, torch.Tensor):
            targets = targets.cpu().numpy()
        
        # è®¡ç®—æ­£è´Ÿæ ·æœ¬æ•°é‡
        positive_count = np.sum(targets == 1)
        negative_count = np.sum(targets == 0)
        total_count = len(targets)
        
        if total_count == 0:
            return
            
        # ä½¿ç”¨æ ‡å‡†çš„ç±»åˆ«ä¸å¹³è¡¡æƒé‡å…¬å¼
        # weight = total_samples / (num_classes * class_count)
        num_classes = 2  # äºŒåˆ†ç±»ï¼šä¸ä¸Šæ¶¨(0) å’Œ ä¸Šæ¶¨(1)
        
        if positive_count > 0 and negative_count > 0:
            # æ ‡å‡†ç±»åˆ«æƒé‡è®¡ç®—
            self.positive_weight = torch.tensor(total_count / (num_classes * positive_count))
            self.negative_weight = torch.tensor(total_count / (num_classes * negative_count))
            
            # é™åˆ¶æƒé‡èŒƒå›´ï¼Œé¿å…è¿‡åº¦ä¸å¹³è¡¡
            max_weight = 5.0
            min_weight = 0.1
            self.positive_weight = torch.clamp(self.positive_weight, min_weight, max_weight)
            self.negative_weight = torch.clamp(self.negative_weight, min_weight, max_weight)
        
    def forward(self, inputs, targets):
        """
        inputs: [batch_size, 1] æ¨¡å‹è¾“å‡ºçš„logits
        targets: [batch_size] çœŸå®æ ‡ç­¾ (0=ä¸ä¸Šæ¶¨, 1=ä¸Šæ¶¨)
        """
        # ç¡®ä¿è¾“å…¥å½¢çŠ¶æ­£ç¡®
        if inputs.dim() == 1:
            inputs = inputs.unsqueeze(1)
        
        # è®¡ç®—BCEæŸå¤±
        bce_loss = F.binary_cross_entropy_with_logits(inputs.squeeze(), targets, reduction='none')
        
        # åº”ç”¨åŠ¨æ€æƒé‡
        weights = torch.where(targets == 1, self.positive_weight, self.negative_weight)
        weighted_loss = weights * bce_loss
        
        if self.reduction == 'mean':
            return weighted_loss.mean()
        elif self.reduction == 'sum':
            return weighted_loss.sum()
        else:
            return weighted_loss


# æ ‡å‡†ä½ç½®ç¼–ç ç±»
class PositionalEncoding(nn.Module):
    """
    æ ‡å‡†çš„æ­£å¼¦ä½ç½®ç¼–ç 
    è®© Transformer è‡ªå·±å­¦ä¹ æ—¶é—´ä¾èµ–å…³ç³»ï¼Œä¸åŠ äººä¸ºè§„åˆ™
    """
    def __init__(self, d_model, max_seq_len=ModelConfig.MAX_SEQ_LEN):
        super(PositionalEncoding, self).__init__()
        
        # åˆ›å»ºæ ‡å‡†çš„æ­£å¼¦/ä½™å¼¦ä½ç½®ç¼–ç 
        pe = torch.zeros(max_seq_len, d_model)
        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        self.register_buffer('pe', pe)
        
    def forward(self, x):
        # ç›´æ¥æ·»åŠ ä½ç½®ç¼–ç ï¼Œä¸ä½¿ç”¨LayerNormï¼ˆä¼šåœ¨åç»­å±‚ä¸­ä½¿ç”¨Pre-Normï¼‰
        seq_len = x.size(1)
        pe_slice = self.pe[:seq_len, :].unsqueeze(0)
        return x + pe_slice

class MultiHeadAttention(nn.Module):
    """
    æ ‡å‡†çš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆPre-Normæ¶æ„ï¼‰
    è®©æ¨¡å‹è‡ªåŠ¨å­¦ä¹ æ¯ä¸ªå¤´åº”è¯¥å…³æ³¨ä»€ä¹ˆç‰¹å¾ï¼Œä¸äººä¸ºå¹²é¢„
    """
    def __init__(self, d_model, nhead):
        super(MultiHeadAttention, self).__init__()
        self.d_model = d_model
        self.nhead = nhead
        
        assert d_model % nhead == 0
        
        # ä½¿ç”¨æ ‡å‡†çš„MultiheadAttention
        self.attention = nn.MultiheadAttention(d_model, nhead, batch_first=True)
        
        # Pre-Norm: åœ¨æ³¨æ„åŠ›ä¹‹å‰è¿›è¡Œå½’ä¸€åŒ–
        self.norm = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(ModelConfig.ATTENTION_DROPOUT)
        
    def forward(self, x, attn_mask=None):
        # Pre-Normæ¶æ„ï¼šå…ˆå½’ä¸€åŒ–ï¼Œå†è®¡ç®—æ³¨æ„åŠ›ï¼Œæœ€åæ®‹å·®è¿æ¥
        # è¾“å‡º = è¾“å…¥ + Dropout(Attention(LayerNorm(è¾“å…¥)))
        
        mask = None
        if attn_mask is not None:
            mask = attn_mask.to(dtype=x.dtype, device=x.device)

        # Pre-Norm: å…ˆå¯¹è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–
        normalized_x = self.norm(x)
        
        # è®¡ç®—æ³¨æ„åŠ›
        attn_output, _ = self.attention(normalized_x, normalized_x, normalized_x, attn_mask=mask)
        
        # æ®‹å·®è¿æ¥ï¼ˆæ³¨æ„è¿™é‡Œæ˜¯åŠ åˆ°åŸå§‹è¾“å…¥xä¸Šï¼Œè€Œä¸æ˜¯normalized_xï¼‰
        output = x + self.dropout(attn_output)
        return output

# æ ‡å‡† Transformer å±‚ï¼ˆPre-Normæ¶æ„ï¼‰
class TransformerLayer(nn.Module):
    """
    æ ‡å‡†çš„ Transformer å±‚ï¼ˆPre-Normæ¶æ„ï¼‰
    è®¾è®¡ç†å¿µï¼šè®©æ¨¡å‹è‡ªåŠ¨å­¦ä¹ åº”è¯¥å…³æ³¨ä»€ä¹ˆç‰¹å¾ï¼Œä¸åŠ äººä¸ºå¹²é¢„
    Pre-Normç›¸æ¯”Post-Normæœ‰æ›´å¥½çš„è®­ç»ƒç¨³å®šæ€§
    """
    def __init__(self, d_model, nhead):
        super(TransformerLayer, self).__init__()
        
        # ä½¿ç”¨Pre-Normå¤šå¤´æ³¨æ„åŠ›
        self.attention = MultiHeadAttention(d_model, nhead)
        
        # å‰é¦ˆç½‘ç»œï¼Œç”¨äºè¿›ä¸€æ­¥å¤„ç†æ³¨æ„åŠ›çš„è¾“å‡º
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_model * 4),  # å…ˆæ‰©å±•ç»´åº¦
            nn.ReLU(),                        # æ¿€æ´»å‡½æ•°
            nn.Dropout(ModelConfig.DROPOUT_RATE),  # é˜²è¿‡æ‹Ÿåˆ
            nn.Linear(d_model * 4, d_model),  # å†å‹ç¼©å›åŸç»´åº¦
        )
        
        # Pre-Norm: åœ¨å‰é¦ˆç½‘ç»œä¹‹å‰è¿›è¡Œå½’ä¸€åŒ–
        self.norm = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(ModelConfig.DROPOUT_RATE)
        
    def forward(self, x):
        # xçš„shape: [batch_size, seq_len, d_model]
        
        # Pre-Normæ¶æ„çš„æ³¨æ„åŠ›å­å±‚ï¼ˆMultiHeadAttentionå†…éƒ¨å·²ç»å®ç°äº†Pre-Normï¼‰
        # è¾“å‡º = è¾“å…¥ + Dropout(Attention(LayerNorm(è¾“å…¥)))
        x = self.attention(x, attn_mask=None)
        
        # Pre-Normæ¶æ„çš„å‰é¦ˆç½‘ç»œå­å±‚
        # è¾“å‡º = è¾“å…¥ + Dropout(FFN(LayerNorm(è¾“å…¥)))
        normalized_x = self.norm(x)
        ff_out = self.feed_forward(normalized_x)
        x = x + self.dropout(ff_out)
        
        return x

# æ ‡å‡† Transformer æ¨¡å‹ï¼ˆPre-Normæ¶æ„ï¼‰
class EnhancedStockTransformer(nn.Module):
    """
    æ ‡å‡† Transformer æ¨¡å‹ï¼ˆPre-Normæ¶æ„ï¼‰ï¼Œç”¨äºè‚¡ç¥¨é¢„æµ‹
    ç§»é™¤äº†äººä¸ºçš„æ—¶é—´è¡°å‡å’Œæ³¨æ„åŠ›æ©ç ï¼Œè®©æ¨¡å‹è‡ªå·±å­¦ä¹ 
    Pre-Normæ¶æ„æä¾›æ›´å¥½çš„è®­ç»ƒç¨³å®šæ€§å’Œæ¢¯åº¦æµ
    """
    def __init__(self, input_dim, d_model, nhead, num_layers, output_dim, max_seq_len):
        super(EnhancedStockTransformer, self).__init__()
        
        self.embedding = nn.Linear(input_dim, d_model)
        
        # ä½¿ç”¨æ ‡å‡†ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)
        
        self.layers = nn.ModuleList([
            TransformerLayer(d_model, nhead) 
            for _ in range(num_layers)
        ])
        
        # Pre-Normæ¶æ„ï¼šåœ¨æœ€åæ·»åŠ ä¸€ä¸ªLayerNorm
        # å› ä¸ºPre-Normçš„æœ€åä¸€å±‚æ²¡æœ‰å½’ä¸€åŒ–è¾“å‡º
        self.final_norm = nn.LayerNorm(d_model)
        
        # ç®€åŒ–è¾“å‡ºå±‚ï¼Œå‡å°‘è¿‡æ‹Ÿåˆ
        self.output_projection = nn.Sequential(
            nn.Linear(d_model, d_model // 2),  # é™ç»´
            nn.ReLU(),
            nn.Dropout(ModelConfig.DROPOUT_RATE),
            nn.Linear(d_model // 2, output_dim)  # æœ€ç»ˆè¾“å‡º
        )
        
        self.dropout = nn.Dropout(ModelConfig.DROPOUT_RATE)
        
    def forward(self, x):
        # 1. ç‰¹å¾åµŒå…¥
        x = self.embedding(x)
        
        # 2. ä½ç½®ç¼–ç 
        x = self.pos_encoding(x)
        x = self.dropout(x)
        
        # 3. Transformerå±‚ï¼ˆPre-Normæ¶æ„ï¼‰
        for layer in self.layers:
            x = layer(x)
        
        # 4. Pre-Normæ¶æ„éœ€è¦åœ¨æœ€åè¿›è¡Œå½’ä¸€åŒ–
        #    å› ä¸ºæ¯å±‚çš„è¾“å‡ºæ²¡æœ‰ç»è¿‡å½’ä¸€åŒ–
        x = self.final_norm(x)
        
        # 5. å–æœ€åæ—¶é—´æ­¥ + è¾“å‡ºæŠ•å½±
        last_hidden = x[:, -1, :]
        output = self.output_projection(last_hidden)
        
        return output

# æ•°æ®é¢„å¤„ç†å‡½æ•°
def load_and_preprocess_data(data_dir=DataConfig.DATA_DIR, test_ratio=DataConfig.TEST_RATIO, seed=DataConfig.RANDOM_SEED):
    """
    æ”¹è¿›çš„æ•°æ®åŠ è½½å’Œé¢„å¤„ç†å‡½æ•°
    ç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†å®Œå…¨ç‹¬ç«‹ï¼Œæ²¡æœ‰æ•°æ®æ³„éœ²
    ä½¿ç”¨å›ºå®šçš„31ä¸ªæµ‹è¯•æ–‡ä»¶ä»¥ç¡®ä¿è¯„ä¼°çš„ä¸€è‡´æ€§
    """
    all_files = [f for f in os.listdir(data_dir) if f.endswith('.xlsx')]
    all_files.sort()  # ç¡®ä¿æ–‡ä»¶é¡ºåºä¸€è‡´
    
    # ä½¿ç”¨å›ºå®šçš„31ä¸ªæµ‹è¯•æ–‡ä»¶ï¼ˆæŒ‰æ–‡ä»¶åæ’åºåçš„å‰31ä¸ªï¼‰
    test_size = 31
    if len(all_files) < test_size:
        print(f"è­¦å‘Š: å¯ç”¨æ–‡ä»¶æ•° ({len(all_files)}) å°‘äº31ä¸ªï¼Œå°†ä½¿ç”¨æ‰€æœ‰æ–‡ä»¶ä½œä¸ºæµ‹è¯•é›†")
        test_size = len(all_files)
    
    test_files = set(all_files[:test_size])  # å›ºå®šä½¿ç”¨å‰31ä¸ªæ–‡ä»¶ä½œä¸ºæµ‹è¯•é›†
    train_files = [f for f in all_files if f not in test_files]
    
    print(f"è®­ç»ƒè‚¡ç¥¨æ–‡ä»¶: {len(train_files)} ä¸ª")
    print(f"æµ‹è¯•è‚¡ç¥¨æ–‡ä»¶: {len(test_files)} ä¸ª (å›ºå®š31ä¸ªæ–‡ä»¶)")
    print(f"æµ‹è¯•æ–‡ä»¶åˆ—è¡¨: {list(test_files)[:5]}...")  # æ˜¾ç¤ºå‰5ä¸ªæµ‹è¯•æ–‡ä»¶

    def process_files(file_list):
        data_list = []
        stock_info_list = []  # æ–°å¢ï¼šå­˜å‚¨è‚¡ç¥¨ä¿¡æ¯
        
        for file in file_list:
            file_path = os.path.join(data_dir, file)
            df = pd.read_excel(file_path)
            try:
                # è·å–æ—¶é—´åˆ—ç”¨äºåˆ¤æ–­2021å¹´
                time_column = df['time'].values
                
                # æ‰¾åˆ°2021å¹´çš„èµ·å§‹ä½ç½®
                year_2021_start = None
                for i, time_str in enumerate(time_column):
                    year = int(time_str.split('/')[0])
                    if year >= 2021:
                        year_2021_start = i
                        break
                
                # å¦‚æœæ²¡æ‰¾åˆ°2021å¹´ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªä½ç½®
                if year_2021_start is None:
                    year_2021_start = len(time_column) - 1
                
                data = df[['start', 'max', 'min', 'end', 'volume']].values
                
                # æ¯åªè‚¡ç¥¨å•ç‹¬æ ‡å‡†åŒ–
                mean = np.mean(data, axis=0)
                std = np.std(data, axis=0)
                if np.any(std == 0):
                    raise ValueError(f"æ–‡ä»¶ {file} åŒ…å«æ ‡å‡†å·®ä¸º0çš„åˆ—")
                normalized_data = (data - mean) / std
                
                data_list.append(normalized_data)
                
                # å­˜å‚¨è‚¡ç¥¨ä¿¡æ¯
                stock_info = {
                    'data_length': len(normalized_data),
                    'year_2021_start': year_2021_start,
                    'file_name': file
                }
                stock_info_list.append(stock_info)
                
            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶ {file} æ—¶å‡ºé”™: {e}")
        
        return data_list, stock_info_list

    train_data, train_stock_info = process_files(train_files)
    test_data, test_stock_info = process_files(test_files)
    
    return train_data, test_data, train_stock_info, test_stock_info

# è®¡ç®—è‚¡ç¥¨é€‰æ‹©æƒé‡
def calculate_stock_weights(stock_info_list):
    """
    è®¡ç®—æ¯åªè‚¡ç¥¨çš„é‡‡æ ·æƒé‡
    æ•°æ®é‡è¶Šå¤§çš„è‚¡ç¥¨æƒé‡è¶Šå¤§ï¼Œä½†æœ€å¤§ä¸è¶…è¿‡å¹³å‡å€¼çš„1.5å€
    """
    data_lengths = [info['data_length'] for info in stock_info_list]
    avg_length = np.mean(data_lengths)
    
    # è®¡ç®—æƒé‡ï¼šæ•°æ®é•¿åº¦ / å¹³å‡é•¿åº¦ï¼Œä½†é™åˆ¶åœ¨1.0åˆ°1.5ä¹‹é—´
    weights = []
    for length in data_lengths:
        weight = length / avg_length
        weight = max(1.0, min(1.5, weight))  # é™åˆ¶åœ¨1.0åˆ°1.5ä¹‹é—´
        weights.append(weight)
    
    # å½’ä¸€åŒ–æƒé‡ï¼Œä½¿å…¶æ€»å’Œä¸º1.0ï¼ˆnp.random.choiceè¦æ±‚ï¼‰
    total_weight = sum(weights)
    normalized_weights = [w / total_weight for w in weights]
    
    return normalized_weights

# æ”¹è¿›çš„æ ·æœ¬ç”Ÿæˆå‡½æ•°
def generate_single_sample_improved(all_data, stock_info_list, stock_weights):
    """
    æ”¹è¿›çš„æ ·æœ¬ç”Ÿæˆå‡½æ•°
    1. æ ¹æ®æ•°æ®é‡å¤§å°é€‰æ‹©è‚¡ç¥¨ï¼ˆæ•°æ®é‡å¤§çš„æ¦‚ç‡æ›´é«˜ï¼‰
    2. é€‰ä¸­è‚¡ç¥¨åï¼Œé€‰æ‹©èµ·å§‹æ—¶é—´åœ¨2021å¹´åæ¦‚ç‡è®¾ç½®ä¸º0.6
    """
    for _ in range(100):  # æœ€å¤šå°è¯•100æ¬¡ç”Ÿæˆæœ‰æ•ˆæ ·æœ¬
        # ç¬¬ä¸€æ­¥ï¼šæ ¹æ®æƒé‡é€‰æ‹©è‚¡ç¥¨
        stock_index = np.random.choice(len(all_data), p=stock_weights)
        stock_data = all_data[stock_index]
        stock_info = stock_info_list[stock_index]
        
        context_length = DataConfig.CONTEXT_LENGTH  # ä½¿ç”¨é…ç½®çš„å†å²æ•°æ®é•¿åº¦
        required_length = DataConfig.REQUIRED_LENGTH  # éœ€è¦é¢å¤–3å¤©æ¥è®¡ç®—æœªæ¥æ”¶ç›Š
        
        if len(stock_data) < required_length:
            continue
            
        # ç¬¬äºŒæ­¥ï¼šé€‰æ‹©èµ·å§‹æ—¶é—´ï¼Œ2021å¹´åæ¦‚ç‡ä¸º0.6
        year_2021_start = stock_info['year_2021_start']
        total_valid_windows = len(stock_data) - required_length + 1
        
        # è®¡ç®—2021å¹´å‰åçš„çª—å£æ•°é‡
        windows_before_2021 = max(0, year_2021_start - required_length + 1)
        windows_after_2021 = total_valid_windows - windows_before_2021
        
        if windows_after_2021 > 0 and windows_before_2021 > 0:
            # æœ‰2021å¹´å‰åçš„æ•°æ®ï¼Œä½¿ç”¨0.6æ¦‚ç‡é€‰æ‹©2021å¹´å
            if np.random.random() < 0.6:
                # é€‰æ‹©2021å¹´åçš„çª—å£
                start_index = np.random.randint(year_2021_start, len(stock_data) - required_length + 1)
            else:
                # é€‰æ‹©2021å¹´å‰çš„çª—å£
                start_index = np.random.randint(0, year_2021_start)
        else:
            # åªæœ‰2021å¹´å‰æˆ–åçš„æ•°æ®ï¼Œéšæœºé€‰æ‹©
            start_index = np.random.randint(0, len(stock_data) - required_length + 1)
        
        input_seq = stock_data[start_index:start_index + context_length]  # 60å¤©å†å²æ•°æ®
        target_seq = stock_data[start_index + context_length:start_index + required_length]  # æœªæ¥3å¤©
        
        # è®¡ç®—æ”¶ç›Šç‡ï¼š(æœªæ¥ä»·æ ¼ - å½“å‰ä»·æ ¼) / å½“å‰ä»·æ ¼
        start_price = input_seq[-1, 3]  # å½“å‰æ”¶ç›˜ä»·ï¼ˆç¬¬3åˆ—æ˜¯endæ”¶ç›˜ä»·ï¼‰
        end_price = target_seq[-1, 3]   # 3å¤©åçš„æ”¶ç›˜ä»·
        
        if start_price == 0:  # é¿å…é™¤é›¶é”™è¯¯
            continue
            
        cumulative_return = (end_price - start_price) / start_price
        
        # äºŒåˆ†ç±»æ ‡ç­¾ï¼šä¸Šæ¶¨ä¸º1ï¼Œä¸ä¸Šæ¶¨ä¸º0
        if cumulative_return >= DataConfig.UPRISE_THRESHOLD:      # æ¶¨å¹…â‰¥2%ï¼šä¸Šæ¶¨
            target = 1.0
        else:                              # å…¶ä»–æƒ…å†µï¼šä¸ä¸Šæ¶¨
            target = 0.0
            
        return input_seq, target
    
    raise ValueError("æ— æ³•ç”Ÿæˆæœ‰æ•ˆæ ·æœ¬ï¼šè‚¡ç¥¨æ•°æ®é•¿åº¦ä¸è¶³æˆ–æ”¶ç›˜ä»·ä¸º0")

def generate_batch_samples_improved(all_data, stock_info_list, stock_weights, batch_size):
    """
    æ”¹è¿›çš„æ‰¹é‡ç”Ÿæˆè®­ç»ƒæ ·æœ¬
    è¿”å›: (batch_inputs, batch_targets)
    batch_inputs: numpy array, shape [batch_size, context_length, 5]  
    batch_targets: numpy array, shape [batch_size]
    """
    batch_inputs = []
    batch_targets = []
    
    attempts = 0
    max_attempts = batch_size * 10  # é˜²æ­¢æ— é™å¾ªç¯
    
    while len(batch_inputs) < batch_size and attempts < max_attempts:
        attempts += 1
        try:
            input_seq, target = generate_single_sample_improved(all_data, stock_info_list, stock_weights)
            batch_inputs.append(input_seq)
            batch_targets.append(target)
        except ValueError:
            continue
    
    if len(batch_inputs) < batch_size:
        raise ValueError(f"æ— æ³•ç”Ÿæˆè¶³å¤Ÿçš„æ ·æœ¬ï¼Œåªç”Ÿæˆäº† {len(batch_inputs)}/{batch_size} ä¸ª")
    
    return np.array(batch_inputs), np.array(batch_targets)

# åˆ›å»ºå›ºå®šçš„è¯„ä¼°æ•°æ®é›†
def create_fixed_evaluation_dataset(test_data, num_samples=DataConfig.EVAL_SAMPLES, seed=DataConfig.RANDOM_SEED):
    """
    åˆ›å»ºå›ºå®šçš„è¯„ä¼°æ•°æ®é›†ï¼Œç¡®ä¿æ¯æ¬¡è¯„ä¼°ä½¿ç”¨ç›¸åŒçš„æ ·æœ¬
    è¿™æ ·å¯ä»¥å‡†ç¡®è¡¡é‡æ¨¡å‹çš„è¿›æ­¥æƒ…å†µ
    ä½¿ç”¨ä¸¥æ ¼çš„éšæœºç§å­æ§åˆ¶ä»¥ç¡®ä¿å®Œå…¨å¯é‡å¤
    """
    print("æ­£åœ¨åˆ›å»ºå›ºå®šçš„è¯„ä¼°æ•°æ®é›†...")
    # è®¾ç½®æ‰€æœ‰å¯èƒ½çš„éšæœºç§å­ä»¥ç¡®ä¿å®Œå…¨å¯é‡å¤
    np.random.seed(seed)
    random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    
    eval_inputs = []
    eval_targets = []
    eval_cumulative_returns = [] # æ–°å¢ï¼šå­˜å‚¨å®é™…æ¶¨è·Œå¹…
    
    # é¢„å…ˆç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„æ ·æœ¬
    all_possible_samples = []
    context_length = DataConfig.CONTEXT_LENGTH
    required_length = DataConfig.REQUIRED_LENGTH
    
    for stock_idx, stock_data in enumerate(test_data):
        if len(stock_data) < required_length:
            continue
            
        # ä¸ºæ¯åªè‚¡ç¥¨ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„æ—¶é—´çª—å£æ ·æœ¬
        for start_idx in range(len(stock_data) - required_length + 1):
            input_seq = stock_data[start_idx:start_idx + context_length]
            target_seq = stock_data[start_idx + context_length:start_idx + required_length]
            
            start_price = input_seq[-1, 3]  # å½“å‰æ”¶ç›˜ä»·
            end_price = target_seq[-1, 3]   # 3å¤©åæ”¶ç›˜ä»·
            
            if start_price == 0:
                continue
                
            cumulative_return = (end_price - start_price) / start_price
            
            # äºŒåˆ†ç±»æ ‡ç­¾ï¼šä¸Šæ¶¨ä¸º1ï¼Œä¸ä¸Šæ¶¨ä¸º0
            if cumulative_return >= DataConfig.UPRISE_THRESHOLD:
                target = 1.0  # ä¸Šæ¶¨
            else:
                target = 0.0  # ä¸ä¸Šæ¶¨
                
            all_possible_samples.append((input_seq, target, stock_idx, start_idx, cumulative_return))
    
    print(f"æ€»å…±å¯ç”¨æ ·æœ¬: {len(all_possible_samples)} ä¸ª")
    
    # éšæœºé€‰æ‹©å›ºå®šçš„è¯„ä¼°æ ·æœ¬
    if len(all_possible_samples) < num_samples:
        print(f"è­¦å‘Š: å¯ç”¨æ ·æœ¬æ•° ({len(all_possible_samples)}) å°‘äºè¯·æ±‚çš„æ ·æœ¬æ•° ({num_samples})")
        selected_samples = all_possible_samples
    else:
        # ä½¿ç”¨å›ºå®šçš„éšæœºç§å­é€‰æ‹©æ ·æœ¬ï¼Œç¡®ä¿æ¯æ¬¡é€‰æ‹©ç›¸åŒçš„æ ·æœ¬
        selected_samples = random.sample(all_possible_samples, num_samples)
    
    # æŒ‰è‚¡ç¥¨ç´¢å¼•å’Œæ—¶é—´ç´¢å¼•æ’åºï¼Œç¡®ä¿é¡ºåºä¸€è‡´
    selected_samples.sort(key=lambda x: (x[2], x[3]))  # æŒ‰è‚¡ç¥¨ç´¢å¼•å’Œæ—¶é—´ç´¢å¼•æ’åº
    
    # åˆ†ç¦»è¾“å…¥å’Œæ ‡ç­¾
    for input_seq, target, stock_idx, start_idx, cumulative_return in selected_samples:
        eval_inputs.append(input_seq)
        eval_targets.append(target)
        eval_cumulative_returns.append(cumulative_return) # ä¿å­˜å®é™…æ¶¨è·Œå¹…
    
    eval_inputs = np.array(eval_inputs)
    eval_targets = np.array(eval_targets)
    eval_cumulative_returns = np.array(eval_cumulative_returns) # è½¬æ¢ä¸ºnumpyæ•°ç»„
    
    # ä¿å­˜è¯„ä¼°æ ·æœ¬ä¿¡æ¯ä»¥ä¾¿è°ƒè¯•
    print(f"è¯„ä¼°æ ·æœ¬è¯¦ç»†ä¿¡æ¯:")
    print(f"  æ ·æœ¬æ€»æ•°: {len(eval_inputs)}")
    print(f"  æ¥è‡ªè‚¡ç¥¨æ•°: {len(set(s[2] for s in selected_samples))}")
    print(f"  æ—¶é—´çª—å£èŒƒå›´: {min(s[3] for s in selected_samples)} - {max(s[3] for s in selected_samples)}")
    
    # æ‰“å°ç±»åˆ«åˆ†å¸ƒ
    unique, counts = np.unique(eval_targets, return_counts=True)
    class_names = ['ä¸ä¸Šæ¶¨', 'ä¸Šæ¶¨']
    print("è¯„ä¼°é›†ç±»åˆ«åˆ†å¸ƒ:")
    for cls, count in zip(unique, counts):
        print(f"  {class_names[int(cls)]}: {count} ä¸ªæ ·æœ¬ ({count/len(eval_targets)*100:.1f}%)")
    
    return eval_inputs, eval_targets, eval_cumulative_returns

# æ‰¹é‡è¯„ä¼°å‡½æ•°
def evaluate_model_batch(model, eval_inputs, eval_targets, eval_cumulative_returns, device, batch_size=DataConfig.EVAL_BATCH_SIZE):
    """
    ä½¿ç”¨æ‰¹å¤„ç†è¿›è¡Œå¿«é€Ÿè¯„ä¼°ï¼ˆäºŒåˆ†ç±»ï¼‰
    è¿”å›: (score, total, class_correct, class_total, pred_positive_correct, pred_positive_total, pred_non_negative, auc_score)
    """
    model.eval()
    score = 0
    total = 0
    class_correct = [0, 0]  # [ä¸ä¸Šæ¶¨æ­£ç¡®æ•°, ä¸Šæ¶¨æ­£ç¡®æ•°]
    class_total = [0, 0]    # [ä¸ä¸Šæ¶¨æ€»æ•°, ä¸Šæ¶¨æ€»æ•°]
    
    # æ–°å¢ï¼šé¢„æµ‹ç»Ÿè®¡
    pred_positive_correct = 0  # é¢„æµ‹ä¸Šæ¶¨ä¸”æ­£ç¡®çš„æ•°é‡
    pred_positive_total = 0    # é¢„æµ‹ä¸Šæ¶¨çš„æ€»æ•°é‡
    pred_non_negative = 0       # é¢„æµ‹ä¸Šæ¶¨ä¸”å®é™…æ¶¨å¹…â‰¥0%çš„æ•°é‡
    
    # æ–°å¢ï¼šç”¨äºAUCè®¡ç®—çš„åˆ—è¡¨
    all_probabilities = []
    all_targets = []
    
    num_samples = len(eval_inputs)
    num_batches = (num_samples + batch_size - 1) // batch_size
    
    with torch.no_grad():
        for i in range(num_batches):
            start_idx = i * batch_size
            end_idx = min((i + 1) * batch_size, num_samples)
            
            # æ‰¹é‡å¤„ç†
            batch_inputs = torch.tensor(eval_inputs[start_idx:end_idx], 
                                      dtype=torch.float32).to(device)
            batch_targets = eval_targets[start_idx:end_idx]
            batch_returns = eval_cumulative_returns[start_idx:end_idx]  # è·å–å®é™…æ¶¨è·Œå¹…
            
            # æ‰¹é‡æ¨ç†
            batch_outputs = model(batch_inputs)  # [batch_size, 1]
            batch_probabilities = torch.sigmoid(batch_outputs).cpu().numpy().flatten()
            batch_predictions = (batch_probabilities > 0.5).astype(int)  # æ¦‚ç‡>0.5é¢„æµ‹ä¸ºä¸Šæ¶¨
            
            # æ”¶é›†æ‰€æœ‰æ¦‚ç‡å’Œæ ‡ç­¾ç”¨äºAUCè®¡ç®—
            all_probabilities.extend(batch_probabilities)
            all_targets.extend(batch_targets)
            
            # æ‰¹é‡è®¡ç®—å¾—åˆ†
            for j in range(len(batch_targets)):
                target = int(batch_targets[j])
                prediction = batch_predictions[j]
                actual_return = batch_returns[j]  # è·å–å®é™…æ¶¨è·Œå¹…
                
                class_total[target] += 1
                total += 1
                
                # ç»Ÿè®¡é¢„æµ‹ä¸Šæ¶¨çš„æƒ…å†µ
                if prediction == 1:
                    pred_positive_total += 1
                    if target == 1:  # é¢„æµ‹ä¸Šæ¶¨ä¸”å®é™…ä¸Šæ¶¨
                        pred_positive_correct += 1
                    if actual_return >= 0:  # é¢„æµ‹ä¸Šæ¶¨ä¸”å®é™…æ¶¨å¹…â‰¥0%
                        pred_non_negative += 1
                
                # åº”ç”¨æ–°çš„è¯„åˆ†è§„åˆ™
                if prediction == 1:  # åªæœ‰é¢„æµ‹ä¸Šæ¶¨æ—¶æ‰è®¡ç®—åˆ†æ•°
                    if actual_return >= 0.02:  # å®é™…ä¸Šæ¶¨â‰¥2%
                        score += EvaluationConfig.UPRISE_CORRECT_HIGH_SCORE
                    elif actual_return >= 0:  # å®é™…æ¶¨0-2%
                        score += EvaluationConfig.UPRISE_CORRECT_LOW_SCORE
                    elif actual_return >= -0.02:  # å®é™…ä¸‹è·Œ<2%
                        score += EvaluationConfig.UPRISE_FALSE_SMALL_PENALTY
                    else:  # å®é™…ä¸‹è·Œâ‰¥2%
                        score += EvaluationConfig.UPRISE_FALSE_LARGE_PENALTY
                
                # ç»Ÿè®¡é¢„æµ‹æ­£ç¡®æ€§ï¼ˆç”¨äºæ˜¾ç¤ºå‡†ç¡®ç‡ï¼Œä¸å½±å“è¯„åˆ†ï¼‰
                if prediction == target:
                    class_correct[target] += 1
    
    # è®¡ç®—AUC
    try:
        auc_score = roc_auc_score(all_targets, all_probabilities)
    except ValueError:
        # å¦‚æœæ‰€æœ‰æ ‡ç­¾éƒ½æ˜¯åŒä¸€ç±»ï¼ŒAUCæ— æ³•è®¡ç®—
        auc_score = 0.5  # éšæœºåˆ†ç±»å™¨çš„AUC
    
    return score, total, class_correct, class_total, pred_positive_correct, pred_positive_total, pred_non_negative, auc_score

def calculate_test_loss(model, eval_inputs, eval_targets, criterion, device, batch_size=DataConfig.EVAL_BATCH_SIZE):
    """
    è®¡ç®—æµ‹è¯•é›†æŸå¤±å€¼
    """
    model.eval()
    total_loss = 0
    num_samples = len(eval_inputs)
    num_batches = (num_samples + batch_size - 1) // batch_size
    
    with torch.no_grad():
        for i in range(num_batches):
            start_idx = i * batch_size
            end_idx = min((i + 1) * batch_size, num_samples)
            
            # æ‰¹é‡å¤„ç†
            batch_inputs = torch.tensor(eval_inputs[start_idx:end_idx], 
                                      dtype=torch.float32).to(device)
            batch_targets = torch.tensor(eval_targets[start_idx:end_idx], 
                                       dtype=torch.float32).to(device)
            
            # è®¡ç®—æŸå¤±
            batch_outputs = model(batch_inputs)
            batch_loss = criterion(batch_outputs, batch_targets)
            total_loss += batch_loss.item()
    
    avg_loss = total_loss / num_batches
    return avg_loss

def print_sample_predictions(model, eval_inputs, eval_targets, device, num_samples=10, epoch=None):
    """
    éšæœºæŒ‘é€‰æ ·æœ¬å¹¶æ‰“å°æ¨¡å‹çš„è¾“å‡ºå€¼ï¼Œç”¨äºè§‚å¯Ÿé¢„æµ‹é›†ä¸­çš„é—®é¢˜
    """
    model.eval()
    
    # éšæœºé€‰æ‹©æ ·æœ¬ç´¢å¼•
    total_samples = len(eval_inputs)
    if num_samples > total_samples:
        num_samples = total_samples
    
    # ä½¿ç”¨å½“å‰epochä½œä¸ºéšæœºç§å­ï¼Œç¡®ä¿æ¯è½®é€‰æ‹©ä¸åŒçš„æ ·æœ¬
    if epoch is not None:
        np.random.seed(DataConfig.RANDOM_SEED + epoch)
    
    sample_indices = np.random.choice(total_samples, size=num_samples, replace=False)
    sample_indices = sorted(sample_indices)  # æ’åºä»¥ä¾¿è§‚å¯Ÿ
    
    print(f"  éšæœºæ ·æœ¬é¢„æµ‹è¯¦æƒ… (ç¬¬{epoch}è½®):")
    print(f"  {'æ ·æœ¬':<4} {'çœŸå®æ ‡ç­¾':<8} {'æ¨¡å‹è¾“å‡º':<12} {'é¢„æµ‹æ¦‚ç‡':<10} {'é¢„æµ‹æ ‡ç­¾':<8} {'é¢„æµ‹ç»“æœ':<8}")
    print(f"  {'-'*4} {'-'*8} {'-'*12} {'-'*10} {'-'*8} {'-'*8}")
    
    with torch.no_grad():
        for i, idx in enumerate(sample_indices):
            # è·å–å•ä¸ªæ ·æœ¬
            sample_input = torch.tensor(eval_inputs[idx:idx+1], dtype=torch.float32).to(device)
            true_label = eval_targets[idx]
            
            # æ¨¡å‹é¢„æµ‹
            model_output = model(sample_input)
            raw_output = model_output.cpu().item()
            probability = torch.sigmoid(model_output).cpu().item()
            predicted_label = 1 if probability > 0.5 else 0
            
            # åˆ¤æ–­é¢„æµ‹ç»“æœ
            if predicted_label == int(true_label):
                result = "âœ“æ­£ç¡®"
            else:
                if int(true_label) == 1 and predicted_label == 0:
                    result = "âœ—æ¼æ¶¨"
                elif int(true_label) == 0 and predicted_label == 1:
                    result = "âœ—è¯¯æ¶¨"
                else:
                    result = "âœ—é”™è¯¯"
            
            # æ ¼å¼åŒ–è¾“å‡º
            true_label_str = "ä¸Šæ¶¨" if int(true_label) == 1 else "ä¸ä¸Šæ¶¨"
            pred_label_str = "ä¸Šæ¶¨" if predicted_label == 1 else "ä¸ä¸Šæ¶¨"
            
            print(f"  {i+1:<4} {true_label_str:<8} {raw_output:<12.4f} {probability:<10.4f} {pred_label_str:<8} {result:<8}")
    
    print()  # ç©ºè¡Œ

# é¢„è®¡ç®—è®­ç»ƒæ•°æ®é›†å‡½æ•°
def precompute_training_dataset(train_data, train_stock_info, train_weights, 
                               batch_size, batches_per_epoch, seed=None):
    """
    é¢„è®¡ç®—æ¯è½®è®­ç»ƒæ‰€éœ€çš„è®­ç»ƒæ•°æ®é›†
    è‡ªåŠ¨æ ¹æ®æ‰¹å¤§å°å’Œæ‰¹æ•°é‡è®¡ç®—éœ€è¦çš„æ ·æœ¬æ•°
    è¿”å›: (epoch_inputs, epoch_targets)
    """
    samples_per_epoch = batch_size * batches_per_epoch
    
    if seed is not None:
        # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
        np.random.seed(seed)
        random.seed(seed)
    
    epoch_inputs = []
    epoch_targets = []
    
    # ç›´æ¥ç”Ÿæˆæ‰€æœ‰éœ€è¦çš„æ ·æœ¬
    epoch_inputs, epoch_targets = generate_batch_samples_improved(
        train_data, train_stock_info, train_weights, samples_per_epoch)
    
    return np.array(epoch_inputs), np.array(epoch_targets)

# æ”¹è¿›çš„è®­ç»ƒå‡½æ•°
def train_model(model, train_data, test_data, train_stock_info, train_weights, epochs=TrainingConfig.EPOCHS, 
               learning_rate=TrainingConfig.LEARNING_RATE, device=None, 
               batch_size=TrainingConfig.BATCH_SIZE, batches_per_epoch=TrainingConfig.BATCHES_PER_EPOCH):
    """
    ä½¿ç”¨é¢„è®¡ç®—è®­ç»ƒæ•°æ®é›†å’Œå›ºå®šè¯„ä¼°é›†çš„è®­ç»ƒå‡½æ•°
    æé«˜è®­ç»ƒæ•ˆç‡ï¼Œç¡®ä¿è¯„ä¼°çš„ä¸€è‡´æ€§
    """
    # è®¾ç½®è®­ç»ƒéšæœºç§å­
    torch.manual_seed(DataConfig.RANDOM_SEED)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(DataConfig.RANDOM_SEED)
        torch.cuda.manual_seed_all(DataConfig.RANDOM_SEED)
    
    # åˆ›å»ºå›ºå®šçš„è¯„ä¼°æ•°æ®é›†ï¼ˆè®­ç»ƒå¼€å§‹å‰åˆ›å»ºä¸€æ¬¡ï¼‰
    eval_inputs, eval_targets, eval_cumulative_returns = create_fixed_evaluation_dataset(test_data, num_samples=DataConfig.EVAL_SAMPLES)
    
    # ä½¿ç”¨åŠ¨æ€åŠ æƒBCEæŸå¤±å‡½æ•°ï¼Œæ ¹æ®æ¯è½®è®­ç»ƒæ•°æ®çš„æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹åŠ¨æ€è°ƒæ•´æƒé‡
    criterion = DynamicWeightedBCE()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=TrainingConfig.WEIGHT_DECAY)
    
    # åˆ›å»ºé¢„çƒ­è°ƒåº¦å™¨å’Œä¸»è°ƒåº¦å™¨
    warmup_scheduler = WarmupScheduler(
        optimizer, 
        warmup_epochs=TrainingConfig.WARMUP_EPOCHS,
        target_lr=learning_rate,
        start_lr=TrainingConfig.WARMUP_START_LR
    )
    
    # æ ¹æ®é…ç½®é€‰æ‹©ä¸»è°ƒåº¦å™¨
    if TrainingConfig.USE_COSINE_ANNEALING:
        main_scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=TrainingConfig.COSINE_T_MAX,
            eta_min=TrainingConfig.COSINE_ETA_MIN
        )
        scheduler_type = "ä½™å¼¦é€€ç«"
    else:
        main_scheduler = optim.lr_scheduler.StepLR(
            optimizer, 
            step_size=TrainingConfig.SCHEDULER_STEP_SIZE, 
            gamma=TrainingConfig.SCHEDULER_GAMMA
        )
        scheduler_type = "é˜¶æ¢¯è¡°å‡"
    
    # æ·»åŠ è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆåŸºäºæ€§èƒ½ï¼‰
    adaptive_scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        mode='max',  # ç›‘æ§å¾—åˆ†æœ€å¤§åŒ–
        factor=TrainingConfig.LR_REDUCE_FACTOR,
        patience=TrainingConfig.PATIENCE,
        min_lr=TrainingConfig.MIN_LR
    )
    
    print(f"å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥: {scheduler_type} + è‡ªé€‚åº”è°ƒæ•´")
    
    best_score = float('-inf')  # æ”¹ç”¨å¾—åˆ†è€Œä¸æ˜¯å‡†ç¡®ç‡
    
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        
        # è®­ç»ƒé˜¶æ®µ - æ›´æ–°å­¦ä¹ ç‡
        if warmup_scheduler.is_warmup_phase():
            # é¢„çƒ­é˜¶æ®µï¼šä½¿ç”¨é¢„çƒ­è°ƒåº¦å™¨
            current_lr = warmup_scheduler.step(epoch)
            lr_status = f"é¢„çƒ­é˜¶æ®µ ({epoch + 1}/{TrainingConfig.WARMUP_EPOCHS})"
        else:
            # é¢„çƒ­ç»“æŸåï¼šä½¿ç”¨ä¸»è°ƒåº¦å™¨
            current_lr = warmup_scheduler.get_last_lr()[0]  # ä¿æŒç›®æ ‡å­¦ä¹ ç‡
            lr_status = "æ­£å¸¸è®­ç»ƒ"
        
        print(f'Epoch {epoch + 1}/{epochs}, LR: {current_lr:.6f} ({lr_status})')
        
        # é¢„è®¡ç®—å½“å‰è½®æ¬¡çš„è®­ç»ƒæ•°æ®
        epoch_seed = DataConfig.RANDOM_SEED + epoch  # æ¯è½®ä½¿ç”¨ä¸åŒçš„ç§å­ç¡®ä¿æ•°æ®å¤šæ ·æ€§
        epoch_inputs, epoch_targets = precompute_training_dataset(
            train_data, train_stock_info, train_weights, batch_size, batches_per_epoch, epoch_seed)
        
        # æ ¹æ®æœ¬è½®è®­ç»ƒæ•°æ®çš„æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹åŠ¨æ€æ›´æ–°æŸå¤±å‡½æ•°æƒé‡
        criterion.update_weights(epoch_targets)
        
        # æ‰“å°æœ¬è½®æƒé‡ä¿¡æ¯
        positive_count = np.sum(epoch_targets == 1)
        negative_count = np.sum(epoch_targets == 0)
        total_count = len(epoch_targets)
        positive_ratio = positive_count / total_count if total_count > 0 else 0
        negative_ratio = negative_count / total_count if total_count > 0 else 0
        
        print(f'  æœ¬è½®æ•°æ®åˆ†å¸ƒ: æ­£æ ·æœ¬={positive_count}({positive_ratio:.1%}), è´Ÿæ ·æœ¬={negative_count}({negative_ratio:.1%})')
        print(f'  åŠ¨æ€æƒé‡: æ­£æ ·æœ¬æƒé‡={criterion.positive_weight.item():.3f}, è´Ÿæ ·æœ¬æƒé‡={criterion.negative_weight.item():.3f}')
        
        # æ˜¾ç¤ºé¢„çƒ­è¿›åº¦å’Œè°ƒåº¦å™¨ä¿¡æ¯
        if warmup_scheduler.is_warmup_phase():
            warmup_progress = (epoch + 1) / TrainingConfig.WARMUP_EPOCHS * 100
            print(f'  é¢„çƒ­è¿›åº¦: {warmup_progress:.1f}% (ç¬¬{epoch + 1}è½®/å…±{TrainingConfig.WARMUP_EPOCHS}è½®)')
            print(f'  å­¦ä¹ ç‡å˜åŒ–: {TrainingConfig.WARMUP_START_LR:.2e} â†’ {current_lr:.2e} â†’ {learning_rate:.2e}(ç›®æ ‡)')
        else:
            # é¢„çƒ­ç»“æŸåæ˜¾ç¤ºå½“å‰è°ƒåº¦å™¨çŠ¶æ€
            if TrainingConfig.USE_COSINE_ANNEALING:
                # è®¡ç®—ä½™å¼¦é€€ç«çš„ç†è®ºå­¦ä¹ ç‡
                import math
                progress = (epoch - TrainingConfig.WARMUP_EPOCHS) / TrainingConfig.COSINE_T_MAX
                theoretical_lr = TrainingConfig.COSINE_ETA_MIN + (learning_rate - TrainingConfig.COSINE_ETA_MIN) * \
                               (1 + math.cos(math.pi * progress)) / 2
                print(f'  ä½™å¼¦é€€ç«è¿›åº¦: {progress*100:.1f}%, ç†è®ºå­¦ä¹ ç‡: {theoretical_lr:.2e}')
            else:
                print(f'  é˜¶æ¢¯è¡°å‡: æ¯{TrainingConfig.SCHEDULER_STEP_SIZE}è½®è¡°å‡{TrainingConfig.SCHEDULER_GAMMA}å€')
        
        # å°†é¢„è®¡ç®—çš„æ•°æ®è½¬æ¢ä¸ºtensorå¹¶ç§»åˆ°è®¾å¤‡ä¸Š
        epoch_inputs_tensor = torch.tensor(epoch_inputs, dtype=torch.float32).to(device)
        epoch_targets_tensor = torch.tensor(epoch_targets, dtype=torch.float32).to(device)
        
        # è®­ç»ƒå¾ªç¯ï¼šä½¿ç”¨é¢„è®¡ç®—çš„æ•°æ®
        for step in range(batches_per_epoch):
            start_idx = step * batch_size
            end_idx = min((step + 1) * batch_size, len(epoch_inputs_tensor))
            
            # ä»é¢„è®¡ç®—çš„æ•°æ®ä¸­å–ä¸€ä¸ªbatch
            batch_inputs = epoch_inputs_tensor[start_idx:end_idx]
            batch_targets = epoch_targets_tensor[start_idx:end_idx]
            
            optimizer.zero_grad()
            output = model(batch_inputs)
            loss = criterion(output, batch_targets)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=TrainingConfig.GRADIENT_CLIP_NORM)
            optimizer.step()
            
            total_loss += loss.item()
            
            # å®æ—¶æ›´æ–°è¿›åº¦æ˜¾ç¤º
            progress = (step + 1) / batches_per_epoch * 100
            avg_loss = total_loss / (step + 1)
            print(f'\r  è®­ç»ƒè¿›åº¦: {progress:.1f}% ({step + 1}/{batches_per_epoch}), å¹³å‡æŸå¤±: {avg_loss:.4f}', end='', flush=True)
        
        print()  # æ¢è¡Œ
        print()  # ç©ºè¡Œ
        
        # æ¸…ç†é¢„è®¡ç®—çš„æ•°æ®ä»¥é‡Šæ”¾å†…å­˜
        del epoch_inputs_tensor, epoch_targets_tensor
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # æ›´æ–°å­¦ä¹ ç‡ï¼ˆåªæœ‰åœ¨é¢„çƒ­ç»“æŸåæ‰ä½¿ç”¨ä¸»è°ƒåº¦å™¨ï¼‰
        if not warmup_scheduler.is_warmup_phase():
            if TrainingConfig.USE_COSINE_ANNEALING:
                main_scheduler.step()  # ä½™å¼¦é€€ç«æŒ‰è½®æ¬¡æ›´æ–°
            else:
                main_scheduler.step()  # StepLRæŒ‰è½®æ¬¡æ›´æ–°
            
            # è‡ªé€‚åº”è°ƒåº¦å™¨æ ¹æ®æ€§èƒ½æ›´æ–°ï¼ˆåœ¨ä¸»è°ƒåº¦å™¨ä¹‹åï¼‰
            old_lr = optimizer.param_groups[0]['lr']
            adaptive_scheduler.step(score)
            new_lr = optimizer.param_groups[0]['lr']
            
            # å¦‚æœå­¦ä¹ ç‡è¢«è‡ªé€‚åº”è°ƒåº¦å™¨é™ä½äº†ï¼Œæ‰“å°ä¿¡æ¯
            if new_lr < old_lr:
                print(f'  ğŸ”½ è‡ªé€‚åº”è°ƒåº¦å™¨è§¦å‘: å­¦ä¹ ç‡ä» {old_lr:.2e} é™ä½åˆ° {new_lr:.2e}')
        
        # å›ºå®šè¯„ä¼°é›†è¯„ä¼°
        score, total, class_correct, class_total, pred_positive_correct, pred_positive_total, pred_non_negative, auc_score = evaluate_model_batch(
            model, eval_inputs, eval_targets, eval_cumulative_returns, device, batch_size=DataConfig.EVAL_BATCH_SIZE
        )
        
        # è®¡ç®—æµ‹è¯•é›†æŸå¤±
        test_loss = calculate_test_loss(model, eval_inputs, eval_targets, criterion, device, batch_size=DataConfig.EVAL_BATCH_SIZE)
        
        # éšæœºæŒ‘é€‰10ç»„æ ·æœ¬æ‰“å°æ¨¡å‹è¾“å‡ºå€¼
        print_sample_predictions(model, eval_inputs, eval_targets, device, num_samples=10, epoch=epoch+1)
        
        # æ‰“å°è¯¦ç»†ç»“æœ
        class_names = ['ä¸ä¸Šæ¶¨', 'ä¸Šæ¶¨']
        for i in range(2):
            if class_total[i] > 0:
                acc = class_correct[i] / class_total[i]
                print(f'  {class_names[i]}: {class_correct[i]}/{class_total[i]} = {acc:.3f}')
            else:
                print(f'  {class_names[i]}: 0/0 = 0.000 (æ— æ ·æœ¬)')
        
        # è®¡ç®—ä¸Šæ¶¨å‡†ç¡®ç‡ï¼ˆé¢„æµ‹ä¸Šæ¶¨åçœŸä¸Šæ¶¨çš„æ¦‚ç‡ï¼‰
        if pred_positive_total > 0:
            precision = pred_positive_correct / pred_positive_total
            non_negative_rate = pred_non_negative / pred_positive_total
            print(f'  ä¸Šæ¶¨å‡†ç¡®ç‡: {pred_positive_correct}/{pred_positive_total} = {precision:.3f} å‡†ç¡®ç‡: {pred_non_negative}/{pred_positive_total} = {non_negative_rate:.3f}')
        else:
            print(f'  ä¸Šæ¶¨å‡†ç¡®ç‡: 0/0 = 0.000 (æ— é¢„æµ‹ä¸Šæ¶¨)')
        
        overall_acc = sum(class_correct) / sum(class_total) if sum(class_total) > 0 else 0
        avg_score = score / total if total > 0 else 0
        
        print(f'  æ€»ä½“å‡†ç¡®ç‡: {overall_acc:.3f}')
        print(f'  è¯„ä¼°å¾—åˆ†: {score} / {total} = {avg_score:.3f}')
        print(f'  AUCå¾—åˆ†: {auc_score:.4f}')
        print(f'  æµ‹è¯•é›†æŸå¤±: {test_loss:.4f}')
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if score > best_score:
            best_score = score
            torch.save(model.state_dict(), ModelSaveConfig.get_best_model_path())
            print(f'  âœ“ å‘ç°æ›´å¥½çš„æ¨¡å‹ï¼å¾—åˆ†æå‡åˆ°: {score}')
        
        print("-" * 50)

if __name__ == "__main__":
    # è®¾ç½®å·¥ä½œç›®å½•
    os.chdir(os.path.dirname(os.path.abspath(__file__)))
    
    # æ‰“å°é…ç½®æ‘˜è¦
    print_config_summary()
    
    # è·å–è®¾å¤‡ä¿¡æ¯
    device = DeviceConfig.print_device_info()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(DataConfig.OUTPUT_DIR, exist_ok=True)
    
    # ä½¿ç”¨æ”¹è¿›çš„æ•°æ®åŠ è½½å‡½æ•°
    print("æ­£åœ¨åŠ è½½å’Œé¢„å¤„ç†æ•°æ®...")
    train_data, test_data, train_stock_info, test_stock_info = load_and_preprocess_data()
    print(f"è®­ç»ƒæ•°æ®: {len(train_data)} åªè‚¡ç¥¨")
    print(f"æµ‹è¯•æ•°æ®: {len(test_data)} åªè‚¡ç¥¨")

    # è®¡ç®—è‚¡ç¥¨é€‰æ‹©æƒé‡
    train_weights = calculate_stock_weights(train_stock_info)
    test_weights = calculate_stock_weights(test_stock_info)
    
    # æ‰“å°æƒé‡ä¿¡æ¯
    print("\nè‚¡ç¥¨é‡‡æ ·æƒé‡ä¿¡æ¯:")
    data_lengths = [info['data_length'] for info in train_stock_info]
    print(f"è®­ç»ƒè‚¡ç¥¨æ•°æ®é•¿åº¦ç»Ÿè®¡:")
    print(f"  æœ€å°é•¿åº¦: {min(data_lengths)}")
    print(f"  æœ€å¤§é•¿åº¦: {max(data_lengths)}")
    print(f"  å¹³å‡é•¿åº¦: {np.mean(data_lengths):.1f}")
    print(f"  æƒé‡èŒƒå›´: {min(train_weights):.3f} - {max(train_weights):.3f}")
    
    # æ˜¾ç¤ºä¸€äº›æ ·æœ¬çš„æƒé‡
    print(f"\nå‰5åªè‚¡ç¥¨çš„æƒé‡ç¤ºä¾‹:")
    for i in range(min(5, len(train_stock_info))):
        info = train_stock_info[i]
        weight = train_weights[i]
        print(f"  {info['file_name']}: æ•°æ®é•¿åº¦={info['data_length']}, æƒé‡={weight:.3f}, 2021å¹´èµ·å§‹ä½ç½®={info['year_2021_start']}")

    print("æ­£åœ¨åˆ›å»º Transformer æ¨¡å‹...")
    model = EnhancedStockTransformer(
        input_dim=ModelConfig.INPUT_DIM, 
        d_model=ModelConfig.D_MODEL, 
        nhead=ModelConfig.NHEAD, 
        num_layers=ModelConfig.NUM_LAYERS, 
        output_dim=ModelConfig.OUTPUT_DIM,
        max_seq_len=ModelConfig.MAX_SEQ_LEN
    ).to(device)
    
    # æ‰“å°æ¨¡å‹å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ¨¡å‹æ€»å‚æ•°æ•°: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°æ•°: {trainable_params:,}")

    print("å¼€å§‹è®­ç»ƒ...")
    # ä½¿ç”¨å¸¦å›ºå®šè¯„ä¼°é›†çš„è®­ç»ƒå‡½æ•°
    train_model(model, train_data, test_data, train_stock_info, train_weights, device=device)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = ModelSaveConfig.get_final_model_path(ModelConfig.D_MODEL)
    torch.save(model.state_dict(), final_model_path)
    print(f"è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_path}")
    print(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {ModelSaveConfig.get_best_model_path()}")